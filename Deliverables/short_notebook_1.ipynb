{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h2o\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../Datasets/ais_train.csv\", delimiter=\"|\")\n",
    "test_data = pd.read_csv(\"../Datasets/ais_test.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vessel_data = pd.read_csv(\"../Datasets/vessels.csv\", delimiter=\"|\")\n",
    "port_data = pd.read_csv(\"../Datasets/ports.csv\", delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"time\"] = pd.to_datetime(train_data[\"time\"])\n",
    "test_data[\"time\"] = pd.to_datetime(test_data[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_data_renamed=pd.DataFrame()\n",
    "port_data_renamed[[\"portId\",\"port_latitude\",\"port_longitude\"]]=port_data[[\"portId\",\"latitude\",\"longitude\"]]\n",
    "train_data=train_data.merge(port_data_renamed, on=\"portId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hy3nmqqx6f70nkbvw0n8lbh80000gn/T/ipykernel_68425/793725113.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group.ffill().bfill())\n"
     ]
    }
   ],
   "source": [
    "train_data_preprocessed = train_data\n",
    "\n",
    "# set the wrong or missing values to nan\n",
    "train_data_preprocessed.loc[train_data_preprocessed[\"cog\"] >= 360, \"cog\"] = np.nan\n",
    "train_data_preprocessed.loc[train_data_preprocessed[\"sog\"] >= 1023, \"sog\"] = np.nan\n",
    "train_data_preprocessed.loc[train_data_preprocessed[\"rot\"] == -128, \"rot\"] = np.nan\n",
    "train_data_preprocessed.loc[train_data_preprocessed[\"heading\"] == 511, \"heading\"] = (\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# extract etaRaw\n",
    "pattern = r\"^\\d{2}-\\d{2} \\d{2}:\\d{2}$\"\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].where(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.match(pattern, na=False), np.nan\n",
    ")\n",
    "\n",
    "\n",
    "train_data_preprocessed = train_data_preprocessed.sort_values(\"time\")\n",
    "\n",
    "\n",
    "# fill and backward fill the missing values within each group\n",
    "train_data_preprocessed = (\n",
    "    train_data_preprocessed.groupby(\"vesselId\")\n",
    "    .apply(lambda group: group.ffill().bfill())\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "train_data_preprocessed[\"heading\"] = train_data_preprocessed[\"heading\"].fillna(0)\n",
    "\n",
    "# drop the nan values which are not filled\n",
    "train_data_preprocessed = train_data_preprocessed.dropna().reset_index(drop=True)\n",
    "\n",
    "## Preprocess the etaRaw colum\n",
    "# Replace '00-' in etaRaw with the corresponding month and day from the 'time' column\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"00-\", na=False),\n",
    "    \"01\" + train_data_preprocessed[\"etaRaw\"].str[2:],\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"-00\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:2]\n",
    "    + \"-01\"\n",
    "    + train_data_preprocessed[\"etaRaw\"].str[5:],\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\":60\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:9] + \"59\",\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"60:\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:6] + \"01:00\",\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"24:\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:6] + \"23:59\",\n",
    ")\n",
    "\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = pd.to_datetime(\n",
    "    train_data_preprocessed[\"time\"].dt.year.astype(str)\n",
    "    + \"-\"\n",
    "    + train_data_preprocessed[\"etaRaw\"]\n",
    "    + \":00\",\n",
    "    format=\"%Y-%m-%d %H:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_preprocessed\n",
      "Index(['time', 'vesselId', 'seconds_to_eta', 'latitude_sin', 'latitude_cos',\n",
      "       'longitude_sin', 'longitude_cos', 'port_latitude_sin',\n",
      "       'port_latitude_cos', 'port_longitude_sin', 'port_longitude_cos',\n",
      "       'cog_sog_sin', 'cog_sog_cos'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# calculate the seconds to eta as a feature\n",
    "train_data_preprocessed[\"seconds_to_eta\"] = (\n",
    "    train_data_preprocessed[\"etaRaw\"] - train_data_preprocessed[\"time\"]\n",
    ").dt.total_seconds()\n",
    "\n",
    "train_data_preprocessed = train_data_preprocessed.drop(columns=[\"etaRaw\"])\n",
    "\n",
    "# calculate the radians of all circular features\n",
    "train_latitude_radians = np.deg2rad(train_data_preprocessed[\"latitude\"])\n",
    "train_longitude_radians = np.deg2rad(train_data_preprocessed[\"longitude\"])\n",
    "train_cog_radians = np.deg2rad(train_data_preprocessed[\"cog\"])\n",
    "train_heading_radians = np.deg2rad(train_data_preprocessed[\"heading\"])\n",
    "\n",
    "port_latitude_radians = np.deg2rad(train_data_preprocessed[\"port_latitude\"])\n",
    "port_longitude_radians = np.deg2rad(train_data_preprocessed[\"port_longitude\"])\n",
    "\n",
    "train_hour = np.deg2rad(train_data_preprocessed[\"time\"].dt.hour * 360 / 24)\n",
    "train_day = np.deg2rad(train_data_preprocessed[\"time\"].dt.day * 360 / 30)\n",
    "train_month = np.deg2rad(train_data_preprocessed[\"time\"].dt.month * 360 / 12)\n",
    "\n",
    "# calculate the sin and cos encodings of the radians\n",
    "train_latitude_sin = np.sin(train_latitude_radians)\n",
    "train_latitude_cos = np.cos(train_latitude_radians)\n",
    "train_longitude_sin = np.sin(train_longitude_radians)\n",
    "train_longitude_cos = np.cos(train_longitude_radians)\n",
    "\n",
    "port_latitude_sin = np.sin(port_latitude_radians)\n",
    "port_latitude_cos = np.cos(port_latitude_radians)\n",
    "port_longitude_sin = np.sin(port_longitude_radians)\n",
    "port_longitude_cos = np.cos(port_longitude_radians)\n",
    "\n",
    "train_cog_sin = np.sin(train_cog_radians)\n",
    "train_cog_cos = np.cos(train_cog_radians)\n",
    "\n",
    "train_heading_sin = np.sin(train_heading_radians)\n",
    "train_heading_cos = np.cos(train_heading_radians)\n",
    "\n",
    "train_hour_sin = np.sin(train_hour)\n",
    "train_hour_cos = np.cos(train_hour)\n",
    "\n",
    "train_day_sin = np.sin(train_day)\n",
    "train_day_cos = np.cos(train_day)\n",
    "\n",
    "train_month_sin = np.sin(train_month)\n",
    "train_month_cos = np.cos(train_month)\n",
    "\n",
    "# generate new features of encodings\n",
    "train_data_preprocessed[\"latitude_sin\"] = train_latitude_sin\n",
    "train_data_preprocessed[\"latitude_cos\"] = train_latitude_cos\n",
    "train_data_preprocessed[\"longitude_sin\"] = train_longitude_sin\n",
    "train_data_preprocessed[\"longitude_cos\"] = train_longitude_cos\n",
    "train_data_preprocessed[\"port_latitude_sin\"] = train_latitude_sin\n",
    "train_data_preprocessed[\"port_latitude_cos\"] = train_latitude_cos\n",
    "train_data_preprocessed[\"port_longitude_sin\"] = train_longitude_sin\n",
    "train_data_preprocessed[\"port_longitude_cos\"] = train_longitude_cos\n",
    "train_data_preprocessed[\"cog_sin\"] = train_cog_sin\n",
    "train_data_preprocessed[\"cog_cos\"] = train_cog_cos\n",
    "train_data_preprocessed[\"heading_sin\"] = train_heading_sin\n",
    "train_data_preprocessed[\"heading_cos\"] = train_heading_cos\n",
    "\n",
    "train_data_preprocessed[\"hour_sin\"] = train_hour_sin\n",
    "train_data_preprocessed[\"hour_cos\"] = train_hour_cos\n",
    "train_data_preprocessed[\"day_sin\"] = train_day_sin\n",
    "train_data_preprocessed[\"day_cos\"] = train_day_cos\n",
    "train_data_preprocessed[\"month_sin\"] = train_month_sin\n",
    "train_data_preprocessed[\"month_cos\"] = train_month_cos\n",
    "\n",
    "# calculate velocity features from cog and sog\n",
    "train_data_preprocessed[\"cog_sog_sin\"] = train_data_preprocessed[\"cog_sin\"]*train_data_preprocessed[\"sog\"]\n",
    "train_data_preprocessed[\"cog_sog_cos\"] = train_data_preprocessed[\"cog_cos\"]*train_data_preprocessed[\"sog\"]\n",
    "\n",
    "# drop the columns which are not needed\n",
    "train_data_preprocessed = train_data_preprocessed.drop(\n",
    "    columns=[\"latitude\", \"longitude\", \"cog\", \"heading\", \"portId\",\"cog_sin\",\"cog_cos\",\"sog\",\"port_latitude\",\"port_longitude\",\"hour_sin\",\"hour_cos\",\"day_sin\",\"day_cos\",\"month_sin\",\"month_cos\",\"rot\",\"heading_sin\",\"heading_cos\",\"navstat\"], axis=1\n",
    ")\n",
    "print(\"train_data_preprocessed\")\n",
    "print(train_data_preprocessed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize the dataframe to use lower memory (32bit)\n",
    "def optimize_dataframe(df):\n",
    "        \"\"\"\n",
    "        Downcasts numerical columns to reduce memory usage.\n",
    "        \"\"\"\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        return df\n",
    "\n",
    "train_data_preprocessed = optimize_dataframe(train_data_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Last_known_location_training_data(\n",
    "    data: pd.DataFrame, max_shift_lengths, max_instances_per_group=1000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"_summary_  Groups training data by vesselId, and propogates all data from last known location\n",
    "\n",
    "    Args:\n",
    "    data (_type_): _description_ the data to be altered\n",
    "\n",
    "    Returns:\n",
    "        _type_:? _description_ the altered data\n",
    "\n",
    "    \"\"\"\n",
    "    all_test_data = pd.DataFrame()\n",
    "    shift_length=1\n",
    "    while shift_length<=max_shift_lengths:\n",
    "\n",
    "        grouped_data = data.groupby(\"vesselId\").apply(lambda x: x.sort_values(\"time\"))\n",
    "\n",
    "        grouped_data[\"time_diff\"] = (\n",
    "            grouped_data[\"time\"].diff(-shift_length).dt.total_seconds().abs()\n",
    "        )\n",
    "\n",
    "        original_time_and_id = grouped_data[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"vesselId\",\n",
    "                \"latitude_sin\",\n",
    "                \"latitude_cos\",\n",
    "                \"longitude_sin\",\n",
    "                \"longitude_cos\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        shifted_data = grouped_data.shift(shift_length)\n",
    "        shifted_data[\n",
    "            [\n",
    "                \"last_latitude_sin\",\n",
    "                \"last_latitude_cos\",\n",
    "                \"last_longitude_sin\",\n",
    "                \"last_longitude_cos\",\n",
    "            ]\n",
    "        ] = shifted_data[\n",
    "            [\"latitude_sin\", \"latitude_cos\", \"longitude_sin\", \"longitude_cos\"]\n",
    "        ]\n",
    "\n",
    "        shifted_data[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"vesselId\",\n",
    "                \"latitude_sin\",\n",
    "                \"latitude_cos\",\n",
    "                \"longitude_sin\",\n",
    "                \"longitude_cos\",\n",
    "            ]\n",
    "        ] = original_time_and_id[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"vesselId\",\n",
    "                \"latitude_sin\",\n",
    "                \"latitude_cos\",\n",
    "                \"longitude_sin\",\n",
    "                \"longitude_cos\",\n",
    "            ]\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # Drops all values with nan values\n",
    "        result = shifted_data.dropna().reset_index(drop=True)\n",
    "\n",
    "        # Define a function to sample or take all if less than max_instances_per_group\n",
    "        def sample_group(group):\n",
    "            if len(group) > max_instances_per_group:\n",
    "                return group.sample(n=max_instances_per_group, random_state=42)\n",
    "            else:\n",
    "                return group\n",
    "\n",
    "        # Apply the sampling function to each group\n",
    "        result = result.groupby('vesselId').apply(sample_group).reset_index(drop=True)\n",
    "\n",
    "        all_test_data = pd.concat([all_test_data, result], ignore_index=True)\n",
    "\n",
    "        prev_shift_length = shift_length\n",
    "        shift_length = int(shift_length**(1.1))\n",
    "        if shift_length == prev_shift_length:\n",
    "            shift_length += 1\n",
    "        print(shift_length)\n",
    "\n",
    "    return all_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data_shifted_df = Last_known_location_training_data(\n",
    "    train_data_preprocessed, 400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_last_known_data_test(\n",
    "    test_data: pd.DataFrame, known_data: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"_summary_  Groups training data by vesselId, and propogates all data from last known location\n",
    "\n",
    "    Args:\n",
    "    data (_type_): _description_ the data to be altered\n",
    "\n",
    "    Returns:\n",
    "        _type_:? _description_ the altered data\n",
    "    \"\"\"\n",
    "\n",
    "    if not test_data[\"vesselId\"].isin(known_data[\"vesselId\"]).all():\n",
    "        missing_vessels = test_data[\n",
    "            ~test_data[\"vesselId\"].isin(known_data[\"vesselId\"])\n",
    "        ][\"vesselId\"].unique()\n",
    "        raise ValueError(\n",
    "            f\"The following vesselIds are missing in known_data: {missing_vessels}\"\n",
    "        )\n",
    "    print(\n",
    "        test_data[~test_data[\"vesselId\"].isin(known_data[\"vesselId\"])][\n",
    "            \"vesselId\"\n",
    "        ].unique()\n",
    "    )\n",
    "\n",
    "    grouped_data = (\n",
    "        known_data.sort_values(\"time\")\n",
    "        .groupby(\"vesselId\")\n",
    "        .tail(1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    original_time = test_data[[\"time\"]]\n",
    "    test_data = test_data.drop(\"time\", axis=1)\n",
    "\n",
    "    result = pd.merge(test_data, grouped_data, how=\"left\", on=\"vesselId\")\n",
    "\n",
    "    result[\"time_diff\"] = (original_time[\"time\"] - result[\"time\"]).dt.total_seconds()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve last known locations for test data\n",
    "test_data_with_last_known_df = append_last_known_data_test(\n",
    "    test_data, train_data_preprocessed\n",
    ")\n",
    "\n",
    "#rename features to match the training data\n",
    "test_data_with_last_known_df[\n",
    "    [\n",
    "        \"last_latitude_sin\",\n",
    "        \"last_latitude_cos\",\n",
    "        \"last_longitude_sin\",\n",
    "        \"last_longitude_cos\",\n",
    "    ]\n",
    "] = test_data_with_last_known_df[\n",
    "    [\n",
    "        \"latitude_sin\",\n",
    "        \"latitude_cos\",\n",
    "        \"longitude_sin\",\n",
    "        \"longitude_cos\",\n",
    "    ]\n",
    "]\n",
    "test_data_with_last_known_df = test_data_with_last_known_df.drop(\n",
    "    columns=[\n",
    "        \"latitude_sin\",\n",
    "        \"latitude_cos\",\n",
    "        \"longitude_sin\",\n",
    "        \"longitude_cos\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_usage_bytes = train_data_shifted_df.memory_usage(deep=True).sum()\n",
    "# Convert bytes to gigabytes\n",
    "memory_usage_gb = memory_usage_bytes / (1024 ** 3)\n",
    "# Print the memory usage in gigabytes\n",
    "print(f\"DataFrame size: {memory_usage_gb:.2f} GB\")\n",
    "\n",
    "h2o.init(max_mem_size=\"32g\") # start h2o with 30GB of memory, alter as needed\n",
    "\n",
    "train_data_shifted = h2o.H2OFrame(train_data_shifted_df)\n",
    "test_data_with_last_known = h2o.H2OFrame(test_data_with_last_known_df)\n",
    "\n",
    "del train_data_shifted_df\n",
    "del test_data_with_last_known_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training data into training and validation data\n",
    "train_data_shifted_without_validation, validation_data_shifted = (\n",
    "    train_data_shifted.split_frame(ratios=[0.9], seed=42)   \n",
    ")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features and target columns, features selected based on feature importance\n",
    "\n",
    "features_lat = [\n",
    "    # \"vesselId\",\n",
    "    \"cog_sog_sin\",\n",
    "    \"cog_sog_cos\",\n",
    "    #\"rot\",\n",
    "    # \"heading_sin\",\n",
    "    # \"heading_cos\",\n",
    "    # \"navstat\",\n",
    "    #\"shippingLineId\",\n",
    "    \"time_diff\",\n",
    "    \"seconds_to_eta\",\n",
    "    \"last_latitude_sin\",\n",
    "    \"last_latitude_cos\",\n",
    "    \"last_longitude_sin\",\n",
    "    \"last_longitude_cos\",\n",
    "    \"port_latitude_sin\",\n",
    "    \"port_latitude_cos\",\n",
    "    \"port_longitude_sin\",\n",
    "    \"port_longitude_cos\",\n",
    "    # \"hour_sin\",\n",
    "    # \"hour_cos\",\n",
    "    #\"day_sin\",\n",
    "    #\"day_cos\",\n",
    "    # \"month_sin\",\n",
    "    # \"month_cos\",\n",
    "]\n",
    "features_long = [\n",
    "    # \"vesselId\",\n",
    "    \"cog_sog_sin\",\n",
    "    \"cog_sog_cos\",\n",
    "    # \"rot\",\n",
    "    # \"shippingLineId\",\n",
    "    # \"heading_sin\",\n",
    "    # \"heading_cos\",\n",
    "    # \"navstat\",\n",
    "    \"time_diff\",\n",
    "    \"seconds_to_eta\",\n",
    "    \"last_latitude_sin\",\n",
    "    \"last_latitude_cos\",\n",
    "    \"last_longitude_sin\",\n",
    "    \"last_longitude_cos\",\n",
    "    \"port_latitude_sin\",\n",
    "    \"port_latitude_cos\",\n",
    "    \"port_longitude_sin\",\n",
    "    \"port_longitude_cos\",\n",
    "    # \"hour_sin\",\n",
    "    # \"hour_cos\",\n",
    "    # \"day_sin\",\n",
    "    # \"day_cos\",\n",
    "    # \"month_sin\",\n",
    "    # \"month_cos\",\n",
    "    \"latitude_sin\",  # append predicted latitude as a feature\n",
    "    \"latitude_cos\",  # append predicted latitude as a feature\n",
    "]\n",
    "\n",
    "# define the target columns\n",
    "target_long_sin = \"longitude_sin\"\n",
    "target_long_cos = \"longitude_cos\"\n",
    "target_lat_sin = \"latitude_sin\"\n",
    "target_lat_cos = \"latitude_cos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the models, found throug grid search\n",
    "params_lat_sin = {\n",
    "    \"ntrees\": 400,  # Maximum number of trees\n",
    "    \"max_depth\": 10,  # Maximum depth of each tree\n",
    "    \"min_rows\": 15,  # Minimum number of rows per leaf\n",
    "    \"sample_rate\": 0.8,  # Row sample rate per tree\n",
    "    \"col_sample_rate\": 0.8,  # Column sample rate per tree\n",
    "    \"reg_lambda\": 1.0,  # L2 regularization term\n",
    "    \"reg_alpha\": 0.1,  # L1 regularization term\n",
    "    \"seed\": 42,  # Random seed for reproducibility\n",
    "    \"stopping_metric\": \"AUTO\",  # Metric for early stopping\n",
    "    \"distribution\": \"gaussian\",  # Set distribution to Gaussian for regression\n",
    "    \"stopping_rounds\": 10,  # Early stopping rounds\n",
    "}\n",
    "params_lat_cos = {\n",
    "    \"ntrees\": 400,  # Maximum number of trees\n",
    "    \"max_depth\": 10,  # Maximum depth of each tree\n",
    "    \"min_rows\": 15,  # Minimum number of rows per leaf\n",
    "    \"sample_rate\": 0.8,  # Row sample rate per tree\n",
    "    \"col_sample_rate\": 0.8,  # Column sample rate per tree\n",
    "    \"reg_lambda\": 1.0,  # L2 regularization term\n",
    "    \"reg_alpha\": 0.0,  # L1 regularization term\n",
    "    \"seed\": 42,  # Random seed for reproducibility\n",
    "    \"stopping_metric\": \"AUTO\",  # Metric for early stopping\n",
    "    \"distribution\": \"gaussian\",  # Set distribution to Gaussian for regression\n",
    "}\n",
    "params_long_sin = {\n",
    "    \"ntrees\": 400,  # Maximum number of trees\n",
    "    \"max_depth\": 10,  # Maximum depth of each tree\n",
    "    \"min_rows\": 15,  # Minimum number of rows per leaf\n",
    "    \"sample_rate\": 0.8,  # Row sample rate per tree\n",
    "    \"col_sample_rate\": 0.8,  # Column sample rate per tree\n",
    "    \"reg_lambda\": 1.0,  # L2 regularization term\n",
    "    \"reg_alpha\": 0.1,  # L1 regularization term\n",
    "    \"seed\": 42,  # Random seed for reproducibility\n",
    "    \"stopping_metric\": \"AUTO\",  # Metric for early stopping\n",
    "    \"distribution\": \"gaussian\",  # Set distribution to Gaussian for regression\n",
    "    \"stopping_rounds\": 10,  # Early stopping rounds\n",
    "}\n",
    "\n",
    "params_long_cos = {\n",
    "    \"ntrees\": 600,  # Maximum number of trees\n",
    "    \"max_depth\": 10,  # Maximum depth of each tree\n",
    "    \"min_rows\": 15,  # Minimum number of rows per leaf\n",
    "    \"sample_rate\": 0.7,  # Row sample rate per tree\n",
    "    \"col_sample_rate\": 0.8,  # Column sample rate per tree\n",
    "    \"reg_lambda\": 1.0,  # L2 regularization term\n",
    "    \"reg_alpha\": 0.1,  # L1 regularization term\n",
    "    \"seed\": 42,  # Random seed for reproducibility\n",
    "    \"stopping_metric\": \"AUTO\",  # Metric for early stopping\n",
    "    \"distribution\": \"gaussian\",  # Set distribution to Gaussian for regression\n",
    "    \"stopping_rounds\": 10,  # Early stopping rounds\n",
    "}\n",
    "\n",
    "gbm_lat_sin = h2o.estimators.H2OXGBoostEstimator(**params_lat_sin)\n",
    "gbm_lat_cos = h2o.estimators.H2OXGBoostEstimator(**params_lat_cos)\n",
    "gbm_long_sin = h2o.estimators.H2OXGBoostEstimator(**params_long_sin)\n",
    "gbm_long_cos = h2o.estimators.H2OXGBoostEstimator(**params_long_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    \"gbm_lat_sin\": {\n",
    "        \"model\": gbm_lat_sin,\n",
    "        \"features\": features_lat,\n",
    "        \"target\": target_lat_sin,\n",
    "    },\n",
    "    \"gbm_lat_cos\": {\n",
    "        \"model\": gbm_lat_cos,\n",
    "        \"features\": features_lat,\n",
    "        \"target\": target_lat_cos,\n",
    "    },\n",
    "    \"gbm_long_sin\": {\n",
    "        \"model\": gbm_long_sin,\n",
    "        \"features\": features_long,  # Ensure features_long is defined\n",
    "        \"target\": target_long_sin,\n",
    "    },\n",
    "    \"gbm_long_cos\": {\n",
    "        \"model\": gbm_long_cos,\n",
    "        \"features\": features_long,  # Ensure features_long is defined\n",
    "        \"target\": target_long_cos,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_lat_sin.train(\n",
    "    x=features_lat,  # .append(latitude_sin)\n",
    "    y=target_lat_sin,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    "    validation_frame=validation_data_shifted,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_lat_cos.train(\n",
    "    x=features_lat,  # .append(latitude_sin)\n",
    "    y=target_lat_cos,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    "    validation_frame=validation_data_shifted,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_lat_sin = gbm_lat_sin.model_performance(test_data=validation_data_shifted)\n",
    "performance_lat_cos = gbm_lat_cos.model_performance(test_data=validation_data_shifted)\n",
    "\n",
    "\n",
    "# Print the performance metrics\n",
    "print(performance_lat_sin)\n",
    "print(performance_lat_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_long_sin.train(\n",
    "    x=features_long,\n",
    "    y=target_long_sin,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    "    validation_frame=validation_data_shifted,\n",
    ")\n",
    "#MSE: 0.00023605270678827214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_long_cos.train(\n",
    "    x=features_long,  # .append(\"longitude_sin\")\n",
    "    y=target_long_cos,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    "    validation_frame=validation_data_shifted,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_long_sin = gbm_long_sin.model_performance(test_data=validation_data_shifted)\n",
    "performance_long_cos = gbm_long_cos.model_performance(test_data=validation_data_shifted)\n",
    "\n",
    "print(performance_long_sin)\n",
    "print(performance_long_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_predicted_lat = test_data_with_last_known\n",
    "\n",
    "lat_predictions_sin = gbm_lat_sin.predict(test_data_with_last_known)\n",
    "test_data_with_last_known[\"latitude_sin\"] = lat_predictions_sin\n",
    "lat_predictions_cos = gbm_lat_cos.predict(test_data_with_last_known)\n",
    "test_data_with_predicted_lat[\"latitude_cos\"] = lat_predictions_cos\n",
    "\n",
    "test_data_with_predicted_lat[\"latitude_sin\"] = lat_predictions_sin\n",
    "test_data_with_predicted_lat[\"latitude_cos\"] = lat_predictions_cos\n",
    "\n",
    "long_predictions_sin = gbm_long_sin.predict(test_data_with_predicted_lat)\n",
    "test_data_with_last_known[\"longitude_sin\"] = long_predictions_sin\n",
    "long_predictions_cos = gbm_long_cos.predict(test_data_with_predicted_lat)\n",
    "test_data_with_last_known[\"longitude_cos\"] = long_predictions_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sine and cosine values back to radians\n",
    "lat_predictions_sin = lat_predictions_sin.as_data_frame()\n",
    "lat_predictions_cos = lat_predictions_cos.as_data_frame()\n",
    "long_predictions_sin = long_predictions_sin.as_data_frame()\n",
    "long_predictions_cos = long_predictions_cos.as_data_frame()\n",
    "\n",
    "\n",
    "lat_predictions_radians = np.arctan2(lat_predictions_sin, lat_predictions_cos)\n",
    "long_predictions_radians = np.arctan2(long_predictions_sin, long_predictions_cos)\n",
    "\n",
    "# Convert radians to degrees\n",
    "lat_predictions_degrees = np.rad2deg(lat_predictions_radians)\n",
    "long_predictions_degrees = np.rad2deg(long_predictions_radians)\n",
    "\n",
    "# Print the first few rows to verify the conversion\n",
    "print(lat_predictions_degrees.head())\n",
    "print(long_predictions_degrees.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prediction_visualization_data(validation_data):\n",
    "    lat_val_sin = gbm_lat_sin.predict(validation_data)\n",
    "    lat_val_cos = gbm_lat_cos.predict(validation_data)\n",
    "    long_val_sin = gbm_long_sin.predict(validation_data)\n",
    "    long_val_cos = gbm_long_cos.predict(validation_data)\n",
    "\n",
    "    lat_val_sin = lat_val_sin.as_data_frame()\n",
    "    lat_val_cos = lat_val_cos.as_data_frame()\n",
    "    long_val_sin = long_val_sin.as_data_frame()\n",
    "    long_val_cos = long_val_cos.as_data_frame()\n",
    "\n",
    "    validation_data = validation_data.as_data_frame()\n",
    "\n",
    "    lat_val_radians = np.arctan2(lat_val_sin, lat_val_cos)\n",
    "    long_val_radians = np.arctan2(long_val_sin, long_val_cos)\n",
    "\n",
    "    evaluation_lat_radians = np.arctan2(\n",
    "        validation_data[\"latitude_sin\"], validation_data[\"latitude_cos\"]\n",
    "    )\n",
    "    evaluation_long_radians = np.arctan2(\n",
    "        validation_data[\"longitude_sin\"], validation_data[\"longitude_cos\"]\n",
    "    )\n",
    "\n",
    "    # Convert radians to degrees\n",
    "    lat_val_degrees = np.rad2deg(lat_val_radians)\n",
    "    long_val_degrees = np.rad2deg(long_val_radians)\n",
    "\n",
    "    evaluation_lat_degrees = np.rad2deg(evaluation_lat_radians)\n",
    "    evaluation_long_degrees = np.rad2deg(evaluation_long_radians)\n",
    "\n",
    "    eval_predictions = pd.concat([lat_val_degrees, long_val_degrees], axis=1)\n",
    "\n",
    "    eval_actual = pd.concat([evaluation_lat_degrees, evaluation_long_degrees], axis=1)\n",
    "\n",
    "    eval_predictions.columns = [\"latitude_predicted\", \"longitude_predicted\"]\n",
    "    eval_actual.columns = [\"latitude\", \"longitude\"]\n",
    "\n",
    "    eval = pd.DataFrame()\n",
    "    eval[[\"latitude_predicted\", \"longitude_predicted\"]] = eval_predictions\n",
    "    eval[[\"latitude\", \"longitude\"]] = eval_actual\n",
    "    eval[[\"vesselId\", \"time\"]] = validation_data[[\"vesselId\", \"time\"]]\n",
    "    eval.to_csv(\"eval_predictions.csv\")\n",
    "\n",
    "\n",
    "# create_prediction_visualization_data(validation_data_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat([lat_predictions_degrees, long_predictions_degrees], axis=1)\n",
    "predictions.columns = [\"latitude_predicted\", \"longitude_predicted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"ID\"] = test_data[\"ID\"]\n",
    "predictions = predictions[[\"ID\", \"longitude_predicted\", \"latitude_predicted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
