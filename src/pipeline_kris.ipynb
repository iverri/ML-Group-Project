{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "import h2o\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# from .feature_engineering_filter import Find_correct_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init(max_mem_size=\"4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = h2o.import_file(\n",
    "    path=\"../Datasets/ais_train.csv\", sep=\"|\", header=1\n",
    ")  # time|cog|sog|rot|heading|navstat|etaRaw|latitude|longitude|vesselId|portId\n",
    "\n",
    "test_data = h2o.import_file(\n",
    "    path=\"../Datasets/ais_test.csv\", sep=\",\", header=1\n",
    ")  # ID,vesselId,time,scaling_factor\n",
    "\n",
    "schedules = h2o.import_file(\n",
    "    path=\"../Datasets/schedules_to_may_2024.csv\", sep=\"|\", header=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .preprocessing import Preprocessing\n",
    "\n",
    "# Implement preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data.as_data_frame()\n",
    "schedules_df = schedules.as_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data=dd.read_csv(\"../Datasets/ais_train.csv\", delimiter=\"|\")\n",
    "test_data=dd.read_csv(\"../Datasets/ais_test.csv\", delimiter=\",\")\n",
    "schedules=dd.read_csv(\"../Datasets/schedules_to_may_2024.csv\", delimiter=\"|\")\n",
    "\n",
    "nan_counts_before = schedules.isnull().sum().compute()\n",
    "print(nan_counts_before)\n",
    "\n",
    "schedules = schedules.ffill()\n",
    "\n",
    "nan_counts_after = schedules.isnull().sum().compute()\n",
    "print(nan_counts_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "from h2o.frame import H2OFrame\n",
    "\n",
    "\n",
    "def append_current_schedule(\n",
    "    queries_frame: H2OFrame, schedules_frame: H2OFrame\n",
    ") -> H2OFrame:\n",
    "    schedules_frame_stripped = schedules_frame[\n",
    "        [\"vesselId\", \"portLatitude\", \"portLongitude\", \"arrivalDate\", \"sailingDate\"]\n",
    "    ]\n",
    "\n",
    "    queries_frame_merged = queries_frame.merge(schedules_frame_stripped)\n",
    "    print(queries_frame_merged.columns)\n",
    "\n",
    "    queries_frame_merged[\"time\"] = queries_frame_merged[\"time\"].as_date(\"%Y-%m-%d %H:%M:%S\")\n",
    "    queries_frame_merged[\"arrivalDate\"] = queries_frame_merged[\"arrivalDate\"].as_date(\"%Y-%m-%d %H:%M:%S\")\n",
    "    queries_frame_merged[\"sailingDate\"] = queries_frame_merged[\"sailingDate\"].as_date(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    queries_frame_filtered = queries_frame_merged[\n",
    "        (queries_frame_merged[\"time\"] <= queries_frame_merged[\"arrivalDate\"])\n",
    "        & (queries_frame_merged[\"time\"] >= queries_frame_merged[\"sailingDate\"])\n",
    "    ]\n",
    "    return queries_frame_filtered\n",
    "\n",
    "\n",
    "# implement feature engineering\n",
    "train_data_appended = append_current_schedule(train_data, schedules)\n",
    "print(train_data_appended.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_current_schedule(queries_frame: dd.DataFrame, schedules_frame: dd.DataFrame) -> dd.DataFrame:\n",
    "\n",
    "    # Convert 'time' to string\n",
    "    # queries_frame[\"time\"] = queries_frame[\"time\"].astype(str)\n",
    "    print(queries_frame[\"time\"].head())\n",
    "\n",
    "    # Strip schedules_frame to keep only the relevant columns\n",
    "    schedules_frame_stripped = schedules_frame[\n",
    "        [\"vesselId\", \"portLatitude\", \"portLongitude\", \"arrivalDate\", \"sailingDate\"]\n",
    "    ]\n",
    "    print(\"Schedules frame stripped to relevant columns\")\n",
    "\n",
    "    # Strip the timezone information (+00:00) from the date columns\n",
    "    schedules_frame_stripped[\"arrivalDate\"] = schedules_frame_stripped[\"arrivalDate\"].astype(str).str.replace(r\"\\+\\d{2}:\\d{2}\", \"\", regex=True)\n",
    "    schedules_frame_stripped[\"sailingDate\"] = schedules_frame_stripped[\"sailingDate\"].astype(str).str.replace(r\"\\+\\d{2}:\\d{2}\", \"\", regex=True)\n",
    "\n",
    "    # Convert timestamps to datetime format\n",
    "    queries_frame[\"time\"] =dd.to_datetime(queries_frame[\"time\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    schedules_frame_stripped[\"arrivalDate\"] = dd.to_datetime(schedules_frame_stripped[\"arrivalDate\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    schedules_frame_stripped[\"sailingDate\"] = dd.to_datetime(schedules_frame_stripped[\"sailingDate\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(\"Converted time and date columns to correct format\")\n",
    "    print(queries_frame[\"time\"].head())\n",
    "    print(schedules_frame_stripped[\"arrivalDate\"].head())\n",
    "\n",
    "    # Sort the DataFrames by time for merge_asof\n",
    "    queries_sorted = queries_frame.sort_values(\"time\")\n",
    "    schedules_sorted = schedules_frame_stripped.sort_values(\"arrivalDate\")\n",
    "\n",
    "    print(schedules_sorted.head())\n",
    "\n",
    "    # Merge the two frames on 'vesselId'\n",
    "    queries_frame_merged = dd.merge_asof(\n",
    "        queries_frame,\n",
    "        schedules_sorted,\n",
    "        left_on=\"time\",\n",
    "        right_on=\"arrivalDate\",\n",
    "        by=\"vesselId\",\n",
    "        direction=\"forward\",\n",
    "    )\n",
    "    print(\"Merged data successfully\")\n",
    "    print(queries_frame_merged.columns)\n",
    "\n",
    "    # Perform filtering based on time range\n",
    "    # print(\"Starting to filter data\")\n",
    "    # queries_frame_filtered = queries_frame_merged[\n",
    "\n",
    "    # (queries_frame_merged[\"time\"] <= queries_frame_merged[\"arrivalDate\"]) &\n",
    "    # (queries_frame_merged[\"time\"] >= queries_frame_merged[\"sailingDate\"])\n",
    "    # ]\n",
    "    # print(\"Successfully filtered the data\")\n",
    "\n",
    "    return queries_frame_merged\n",
    "\n",
    "# Execute the feature engineering function\n",
    "print(\"Displaying the head of the schedules frame for reference:\")\n",
    "\n",
    "# Assume 'train_data' and 'schedules' are your pandas DataFrames\n",
    "train_data_appended = append_current_schedule(test_data, schedules)\n",
    "\n",
    "print(\"sucsessfully generated data, attempting to store\")\n",
    "\n",
    "# Export the filtered frame to a CSV file\n",
    "train_data_appended.compute().to_csv(\"intermediate/test_data_with_schedule_whole.csv\", index=False)\n",
    "\n",
    "nan_counts_after = train_data_appended.isnull().sum().compute()\n",
    "print(nan_counts_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_schedule=pd.read_csv(\"intermediate/train_data_with_schedule_whole.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_schedule=pd.read_csv(\"intermediate/test_data_with_schedule_whole.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_data=pd.read_csv(\"../Datasets/ports.csv\", delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_with_schedule.columns)\n",
    "train_data_with_schedule=train_data_with_schedule[[\"time\",\"latitude\",\"longitude\",\"vesselId\",\"portLatitude\",\"portLongitude\",\"arrivalDate\"]]\n",
    "train_data_without_schedule=train_data_with_schedule[[\"time\",\"vesselId\",\"latitude\",\"longitude\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_schedule=test_data_with_schedule[[\"time\",\"vesselId\",\"portLatitude\",\"portLongitude\",\"arrivalDate\"]]\n",
    "test_data_without_schedule=test_data_with_schedule[[\"time\",\"vesselId\"]]\n",
    "test_data_with_schedule[\"id\"]=test_data_without_schedule.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_schedule=train_data_with_schedule.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_schedule=test_data_with_schedule.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert H2O Frame to pandas DataFrame\n",
    "#train_data_df = train_data.as_data_frame()\n",
    "# Convert 'timestamp' column to datetime\n",
    "train_data_with_schedule[\"time\"] = pd.to_datetime(train_data_with_schedule[\"time\"]).astype(int)//10**9\n",
    "train_data_with_schedule[\"arrivalDate\"] = pd.to_datetime(train_data_with_schedule[\"arrivalDate\"]).astype(int)//10**9\n",
    "train_data_without_schedule[\"time\"] = pd.to_datetime(train_data_without_schedule[\"time\"]).astype(int)//10**9\n",
    "\n",
    "\n",
    "# Convert pandas DataFrame back to H2O Frame\n",
    "train_data_with_schedule = h2o.H2OFrame(train_data_with_schedule)\n",
    "train_data_without_schedule = h2o.H2OFrame(train_data_with_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert H2O Frame to pandas DataFrame\n",
    "#test_data_df = test_data.as_data_frame()\n",
    "\n",
    "# Convert 'timestamp' column to datetime\n",
    "test_data_with_schedule[\"time\"] = pd.to_datetime(test_data_with_schedule[\"time\"]).astype(int) // 10**9\n",
    "test_data_without_schedule[\"time\"] = pd.to_datetime(test_data_without_schedule[\"time\"]).astype(int) // 10**9\n",
    "\n",
    "# Convert pandas DataFrame back to H2O Frame\n",
    "test_data_with_schedule = h2o.H2OFrame(test_data_with_schedule)\n",
    "test_data_without_schedule = h2o.H2OFrame(test_data_without_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_with_schedule.head())\n",
    "print(test_data_without_schedule.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = train_data.split_frame(\n",
    "    ratios=[0.7], seed=1\n",
    ")  # 70% for train_data, 30% for validation_data\n",
    "\n",
    "# create a test subset from train data\n",
    "train_data = splits[0]\n",
    "validation_data = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_with_schedule.columns)\n",
    "#test_data = test_data.drop(\"ID\", axis=1)\n",
    "test_data = test_data.drop(\"scaling_factor\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_without_schedule = [\n",
    "    \"time\",\n",
    "    \"vesselId\",\n",
    "]\n",
    "features_with_schedule = [\n",
    "    \"time\",\"vesselId\",\"portLatitude\",\"portLongitude\",\"arrivalDate\"\n",
    "]\n",
    "target_long = \"longitude\"  # replace with your actual target column\n",
    "target_lat = \"latitude\"  # replace with your actual target column\n",
    "\n",
    "# target_cog = \"cog\"\n",
    "# target_sog = \"sog\"\n",
    "# target_rot = \"rot\"\n",
    "# target_heading = \"heading\"\n",
    "# target_navstat = \"navstat\"\n",
    "# # target_etaRaw = \"etaRaw\" #Remove etaRaw because it requires preprocessing\n",
    "# target_portId = \"portId\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\"ntrees\": [50, 100, 200], \"learn_rate\": [0.01, 0.1, 0.2, 0.3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_lat_with_schedule = h2o.estimators.H2OXGBoostEstimator()\n",
    "gbm_long_with_schedule = h2o.estimators.H2OXGBoostEstimator()\n",
    "gbm_lat_without_schedule = h2o.estimators.H2OXGBoostEstimator()\n",
    "gbm_long_without_schedule = h2o.estimators.H2OXGBoostEstimator()\n",
    "\n",
    "# gbm_cog = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_sog = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_rot = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_heading = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_navstat = h2o.estimators.H2OXGBoostEstimator()\n",
    "# # gbm_etaRaw = h2o.esti#mators.H2OXGBoostEstimator() #Remove etaRaw because it requires preprocessing\n",
    "# # gbm_portId = h2o.estimators.H2OXGBoostEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_cog.train(x=features_test, y=target_cog, training_frame=train_data)\n",
    "\n",
    "cog_mse = gbm_cog.model_performance(validation_data).mse()\n",
    "print(\"cog_mse: \", cog_mse)\n",
    "cog_predicted = gbm_cog.predict(test_data)\n",
    "\n",
    "\n",
    "h2o.remove(gbm_cog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_sog.train(x=features_test, y=target_sog, training_frame=train_data)\n",
    "sog_mse = gbm_sog.model_performance(validation_data).mse()\n",
    "print(\"sog_mse: \", sog_mse)\n",
    "sog_predicted = gbm_sog.predict(test_data)\n",
    "h2o.remove(gbm_sog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_rot.train(x=features_test, y=target_rot, training_frame=train_data)\n",
    "rot_mse = gbm_rot.model_performance(validation_data).mse()\n",
    "print(\"rot_mse: \", rot_mse)\n",
    "rot_predicted = gbm_rot.predict(test_data)\n",
    "h2o.remove(gbm_rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_heading.train(x=features_test, y=target_heading, training_frame=train_data)\n",
    "heading_mse = gbm_heading.model_performance(validation_data).mse()\n",
    "print(\"heading_mse: \", heading_mse)\n",
    "heading_predicted = gbm_heading.predict(test_data)\n",
    "h2o.remove(gbm_heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[target_navstat] = train_data[target_navstat].asfactor()\n",
    "\n",
    "gbm_navstat.train(x=features_test, y=target_navstat, training_frame=train_data)\n",
    "navstat_performance = gbm_navstat.model_performance(validation_data)\n",
    "navstat_predicted = gbm_navstat.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[target_navstat] = validation_data[target_navstat].asfactor()\n",
    "navstat_performance = gbm_navstat.model_performance(validation_data)\n",
    "\n",
    "print(\"navstat_precision: \", navstat_performance.mse())\n",
    "print(\"navstat_recall: \", navstat_performance.recall())\n",
    "print(\"navstat_f1: \", navstat_performance.F1())\n",
    "print(\"navstat_auc: \", navstat_performance.auc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbm_etaRaw.train(x=features_test, y=target_etaRaw, training_frame=train_data)\n",
    "# etaRaw_predicted = gbm_etaRaw.predict(test_data)\n",
    "# h2o.remove(gbm_etaRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbm_portId.train(x=features_test, y=target_portId, training_frame=train_data)\n",
    "# portId_predicted = gbm_portId.predict(test_data)\n",
    "# h2o.remove(gbm_portId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gbm_long_with_schedule.train(x=features_with_schedule, y=target_long, training_frame=train_data_with_schedule)\n",
    "gbm_lat_with_schedule.train(x=features_with_schedule, y=target_lat, training_frame=train_data_with_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_long_without_schedule.train(x=features_without_schedule, y=target_long, training_frame=train_data_without_schedule)\n",
    "gbm_lat_without_schedule.train(x=features_without_schedule, y=target_lat, training_frame=train_data_without_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cog_predicted.set_names([\"cog\"])\n",
    "sog_predicted.set_names([\"sog\"])\n",
    "rot_predicted.set_names([\"rot\"])\n",
    "heading_predicted.set_names([\"heading\"])\n",
    "navstat_predicted.set_names([\"navstat\"])\n",
    "# etaRaw_predicted.set_names([\"etaRaw\"])\n",
    "# portId_predicted.set_names([\"portId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_predicted = test_data\n",
    "test_data_predicted = test_data.cbind(cog_predicted)\n",
    "test_data_predicted = test_data.cbind(sog_predicted)\n",
    "test_data_predicted = test_data.cbind(rot_predicted)\n",
    "test_data_predicted = test_data.cbind(heading_predicted)\n",
    "test_data_predicted = test_data.cbind(navstat_predicted)\n",
    "# test_data_predicted = test_data.cbind(etaRaw_predicted)\n",
    "# test_data_predicted = test_data.cbind(portId_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_lat = h2o.grid.grid_search.H2OGridSearch(gbm_lat, hyper_params)\n",
    "# grid_lat.train(x=features, y=target_lat, training_frame=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridperf = grid_lat.get_grid(sort_by=\"mse\", decreasing=True)\n",
    "\n",
    "# gbm_lat = gridperf.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gbm_lat.params[\"learn_rate\"][\"actual\"])\n",
    "# print(gbm_lat.params[\"ntrees\"][\"actual\"])\n",
    "\n",
    "# 0.01\n",
    "# 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbm_long = h2o.estimators.H2OXGBoostEstimator(\n",
    "#     learn_rate=gbm_lat.params[\"learn_rate\"][\"actual\"],\n",
    "#     ntrees=gbm_lat.params[\"ntrees\"][\"actual\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbm_long.train(x=features, y=target_long, training_frame=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n",
      "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "lat_predictions_with_schedule = gbm_lat_with_schedule.predict(test_data_with_schedule)\n",
    "long_predictions_with_schedule = gbm_long_with_schedule.predict(test_data_with_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n",
      "xgboost prediction progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/job.py:81: UserWarning: Test/Validation dataset column 'vesselId' has levels not trained on: [\"61e9f38eb937134a3c4bfd8d\", \"61e9f3a0b937134a3c4bfdd1\", \"61e9f3a8b937134a3c4bfdfd\", \"61e9f3aab937134a3c4bfe11\", \"61e9f3bcb937134a3c4bfe91\", \"61e9f3bfb937134a3c4bfe9d\", \"61e9f3c5b937134a3c4bfec5\", \"61e9f3c5b937134a3c4bfecb\", \"61e9f3c6b937134a3c4bfecf\", \"61e9f3c6b937134a3c4bfed3\", ...129 not listed..., \"6326eed6c46d6a20d22ca319\", \"6326f0e8c46d6a20d22ca31c\", \"6326f5a6c46d6a20d22ca31e\", \"6326fdeac46d6a20d22ca324\", \"63d27587e3fba838ce820405\", \"clh6aqawa0001gh0zmijpuho1\", \"clh6aqawa0002gh0zypfa5dut\", \"clh6aqawa0004gh0z12aogec9\", \"clh6aqawa0006gh0zje911dl3\", \"clh6aqawa0007gh0z9h6zi9bo\"]\n",
      "  warnings.warn(w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "███████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "lat_predictions_without_schedule = gbm_lat_without_schedule.predict(test_data_without_schedule)\n",
    "long_predictions_without_schedule = gbm_long_without_schedule.predict(test_data_without_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/frame.py:1981: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n",
      "/Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/frame.py:1981: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n",
      "/Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/frame.py:1981: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n",
      "/Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/frame.py:1981: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
     ]
    }
   ],
   "source": [
    "lat_predictions_with_schedule = lat_predictions_with_schedule.as_data_frame()\n",
    "long_predictions_with_schedule = long_predictions_with_schedule.as_data_frame()\n",
    "lat_predictions_without_schedule = lat_predictions_without_schedule.as_data_frame()\n",
    "long_predictions_without_schedule = long_predictions_without_schedule.as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_with_schedule_df = pd.concat([lat_predictions_with_schedule, long_predictions_with_schedule], axis=1)\n",
    "predictions_with_schedule_df.columns = [\"latitude_predicted\", \"longitude_predicted\"]\n",
    "predictions_without_schedule_df = pd.concat([lat_predictions_without_schedule, long_predictions_without_schedule], axis=1)\n",
    "predictions_without_schedule_df.columns = [\"latitude_predicted\", \"longitude_predicted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/frame.py:1981: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
     ]
    }
   ],
   "source": [
    "predictions_with_schedule_df[\"id\"]=test_data_with_schedule.as_data_frame()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_with_schedule_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_without_schedule_df.update(predictions_with_schedule_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_without_schedule_df[\"ID\"] = predictions_without_schedule_df.index\n",
    "# set the ID as the first column\n",
    "predictions_without_schedule_df = predictions_without_schedule_df[[\"ID\", \"longitude_predicted\", \"latitude_predicted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'longitude_predicted', 'latitude_predicted'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(predictions_without_schedule_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_without_schedule_df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
