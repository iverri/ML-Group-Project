{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import h2o\n",
    "import os\n",
    "import pandas as pd\n",
    "# from .feature_engineering_filter import Find_correct_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../Datasets/ais_train.csv\", delimiter=\"|\")\n",
    "test_data = pd.read_csv(\"../Datasets/ais_test.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"time\"] = pd.to_datetime(train_data[\"time\"])\n",
    "test_data[\"time\"] = pd.to_datetime(test_data[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time    cog   sog  rot  heading  navstat       etaRaw  \\\n",
      "0 2024-01-01 00:00:25  284.0   0.7  0.0     88.0        0  01-09 23:00   \n",
      "1 2024-01-01 00:00:36  109.6   0.0 -6.0    347.0        1  12-29 20:00   \n",
      "2 2024-01-01 00:01:45  111.0  11.0  0.0    112.0        0  01-02 09:00   \n",
      "3 2024-01-01 00:03:11   96.4   0.0  0.0    142.0        1  12-31 20:00   \n",
      "4 2024-01-01 00:03:51  214.0  19.7  0.0    215.0        0  01-25 12:00   \n",
      "\n",
      "   latitude  longitude                  vesselId                    portId  \n",
      "0 -34.74370  -57.85130  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f  \n",
      "1   8.89440  -79.47939  61e9f3d4b937134a3c4bff1f  634c4de270937fc01c3a7689  \n",
      "2  39.19065  -76.47567  61e9f436b937134a3c4c0131  61d3847bb7b7526e1adf3d19  \n",
      "3 -34.41189  151.02067  61e9f3b4b937134a3c4bfe77  61d36f770a1807568ff9a126  \n",
      "4  35.88379   -5.91636  61e9f41bb937134a3c4c0087  634c4de270937fc01c3a74f3  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_25422/832757499.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_data_preprocessed = train_data_preprocessed.groupby(\"vesselId\").apply(lambda group: group.ffill().bfill()).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time    cog   sog  rot  heading  navstat       etaRaw  \\\n",
      "0 2024-01-12 14:07:47  308.1  17.1 -6.0    316.0        0  01-08 06:00   \n",
      "1 2024-01-12 14:31:00  307.6  17.3  5.0    313.0        0  01-14 23:30   \n",
      "2 2024-01-12 14:57:23  306.8  16.9  5.0    312.0        0  01-14 23:30   \n",
      "3 2024-01-12 15:18:48  307.9  16.9  6.0    313.0        0  01-14 23:30   \n",
      "4 2024-01-12 15:39:47  307.0  16.3  7.0    313.0        0  01-14 23:30   \n",
      "\n",
      "   latitude  longitude                  vesselId                    portId  \n",
      "0   7.50361   77.58340  61e9f38eb937134a3c4bfd8b  61d376b393c6feb83e5eb50c  \n",
      "1   7.57302   77.49505  61e9f38eb937134a3c4bfd8b  61d376d893c6feb83e5eb546  \n",
      "2   7.65043   77.39404  61e9f38eb937134a3c4bfd8b  61d376d893c6feb83e5eb546  \n",
      "3   7.71275   77.31394  61e9f38eb937134a3c4bfd8b  61d376d893c6feb83e5eb546  \n",
      "4   7.77191   77.23585  61e9f38eb937134a3c4bfd8b  61d376d893c6feb83e5eb546  \n"
     ]
    }
   ],
   "source": [
    "train_data_preprocessed = train_data\n",
    "train_data_preprocessed.loc[train_data_preprocessed[\"cog\"] >= 360, \"cog\"] = np.nan\n",
    "train_data_preprocessed.loc[train_data_preprocessed[\"sog\"] >= 1023, \"sog\"] = np.nan\n",
    "train_data_preprocessed.loc[train_data_preprocessed[\"rot\"] == -128, \"rot\"] = np.nan\n",
    "train_data_preprocessed.loc[train_data_preprocessed[\"heading\"] == 511, \"heading\"] = np.nan\n",
    "\n",
    "\n",
    "pattern = r\"^\\d{2}-\\d{2} \\d{2}:\\d{2}$\"\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].where(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.match(pattern, na=False), np.nan\n",
    ")\n",
    "\n",
    "\n",
    "train_data_preprocessed=train_data_preprocessed.sort_values(\"time\")\n",
    "\n",
    "print(train_data_preprocessed.head())\n",
    "\n",
    "\n",
    "train_data_preprocessed = train_data_preprocessed.groupby(\"vesselId\").apply(lambda group: group.ffill().bfill()).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(train_data_preprocessed.head())\n",
    "\n",
    "train_data_preprocessed[\"heading\"] = train_data_preprocessed[\"heading\"].fillna(0)\n",
    "\n",
    "train_data_preprocessed = train_data_preprocessed.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Replace '00-' in etaRaw with the corresponding month and day from the 'time' column\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"00-\", na=False),\n",
    "    \"01\"\n",
    "    + train_data_preprocessed[\"etaRaw\"].str[2:],\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"-00\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:2]\n",
    "    +\"-01\"\n",
    "    + train_data_preprocessed[\"etaRaw\"].str[5:],\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\":60\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:9]+\"59\",\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"60:\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:6]+\"01:00\",\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"24:\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:6]+\"23:59\"\n",
    ")\n",
    "\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = pd.to_datetime(\n",
    "    train_data_preprocessed[\"time\"].dt.year.astype(str)\n",
    "    + \"-\"\n",
    "    + train_data_preprocessed[\"etaRaw\"]\n",
    "    + \":00\",\n",
    "    format=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "\n",
    "train_data_preprocessed[\"seconds_to_eta\"] = (\n",
    "    train_data_preprocessed[\"etaRaw\"]-train_data_preprocessed[\"time\"]\n",
    ").dt.total_seconds()\n",
    "\n",
    "train_data_preprocessed = train_data_preprocessed.drop(columns=[\"etaRaw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7.50361\n",
      "1    7.57302\n",
      "2    7.65043\n",
      "3    7.71275\n",
      "4    7.77191\n",
      "5    7.81285\n",
      "6    7.86929\n",
      "7    7.92585\n",
      "8    7.98258\n",
      "9    8.03598\n",
      "Name: latitude, dtype: float64\n",
      "0    0.130963\n",
      "1    0.132174\n",
      "2    0.133525\n",
      "3    0.134613\n",
      "4    0.135645\n",
      "5    0.136360\n",
      "6    0.137345\n",
      "7    0.138332\n",
      "8    0.139322\n",
      "9    0.140254\n",
      "Name: latitude, dtype: float64\n",
      "                 time   sog  rot  navstat                  vesselId  \\\n",
      "0 2024-01-12 14:07:47  17.1 -6.0        0  61e9f38eb937134a3c4bfd8b   \n",
      "1 2024-01-12 14:31:00  17.3  5.0        0  61e9f38eb937134a3c4bfd8b   \n",
      "2 2024-01-12 14:57:23  16.9  5.0        0  61e9f38eb937134a3c4bfd8b   \n",
      "3 2024-01-12 15:18:48  16.9  6.0        0  61e9f38eb937134a3c4bfd8b   \n",
      "4 2024-01-12 15:39:47  16.3  7.0        0  61e9f38eb937134a3c4bfd8b   \n",
      "5 2024-01-12 15:54:48  16.1  5.0        0  61e9f38eb937134a3c4bfd8b   \n",
      "6 2024-01-12 16:14:59  16.1 -6.0        0  61e9f38eb937134a3c4bfd8b   \n",
      "7 2024-01-12 16:35:24  16.0  2.0        0  61e9f38eb937134a3c4bfd8b   \n",
      "8 2024-01-12 16:55:24  16.0 -1.0        0  61e9f38eb937134a3c4bfd8b   \n",
      "9 2024-01-12 17:14:36  16.1  6.0        0  61e9f38eb937134a3c4bfd8b   \n",
      "\n",
      "   seconds_to_eta  latitude_sin  latitude_cos  longitude_sin  longitude_cos  \\\n",
      "0       -374867.0      0.130589      0.991437       0.976610       0.215018   \n",
      "1        205140.0      0.131790      0.991278       0.976277       0.216524   \n",
      "2        203557.0      0.133129      0.991099       0.975894       0.218245   \n",
      "3        202272.0      0.134207      0.990953       0.975588       0.219609   \n",
      "4        201013.0      0.135230      0.990814       0.975288       0.220938   \n",
      "5        200112.0      0.135938      0.990717       0.975078       0.221864   \n",
      "6        198901.0      0.136914      0.990583       0.974801       0.223075   \n",
      "7        197676.0      0.137891      0.990447       0.974519       0.224303   \n",
      "8        196476.0      0.138872      0.990310       0.974247       0.225482   \n",
      "9        195324.0      0.139795      0.990180       0.973980       0.226635   \n",
      "\n",
      "    cog_sin   cog_cos  heading_sin  heading_cos  hour_sin  hour_cos   day_sin  \\\n",
      "0 -0.786935  0.617036    -0.694658     0.719340 -0.500000 -0.866025  0.587785   \n",
      "1 -0.792290  0.610145    -0.731354     0.681998 -0.500000 -0.866025  0.587785   \n",
      "2 -0.800731  0.599024    -0.743145     0.669131 -0.500000 -0.866025  0.587785   \n",
      "3 -0.789084  0.614285    -0.731354     0.681998 -0.707107 -0.707107  0.587785   \n",
      "4 -0.798636  0.601815    -0.731354     0.681998 -0.707107 -0.707107  0.587785   \n",
      "5 -0.792290  0.610145    -0.731354     0.681998 -0.707107 -0.707107  0.587785   \n",
      "6 -0.771625  0.636078    -0.731354     0.681998 -0.866025 -0.500000  0.587785   \n",
      "7 -0.780430  0.625243    -0.754710     0.656059 -0.866025 -0.500000  0.587785   \n",
      "8 -0.761538  0.648120    -0.754710     0.656059 -0.866025 -0.500000  0.587785   \n",
      "9 -0.793353  0.608761    -0.798636     0.601815 -0.965926 -0.258819  0.587785   \n",
      "\n",
      "    day_cos  month_sin  month_cos  \n",
      "0 -0.809017        0.5   0.866025  \n",
      "1 -0.809017        0.5   0.866025  \n",
      "2 -0.809017        0.5   0.866025  \n",
      "3 -0.809017        0.5   0.866025  \n",
      "4 -0.809017        0.5   0.866025  \n",
      "5 -0.809017        0.5   0.866025  \n",
      "6 -0.809017        0.5   0.866025  \n",
      "7 -0.809017        0.5   0.866025  \n",
      "8 -0.809017        0.5   0.866025  \n",
      "9 -0.809017        0.5   0.866025  \n"
     ]
    }
   ],
   "source": [
    "train_latitude_radians = np.deg2rad(train_data_preprocessed[\"latitude\"])\n",
    "train_longitude_radians = np.deg2rad(train_data_preprocessed[\"longitude\"])\n",
    "train_cog_radians = np.deg2rad(train_data_preprocessed[\"cog\"])\n",
    "train_heading_radians = np.deg2rad(train_data_preprocessed[\"heading\"])\n",
    "train_hour = np.deg2rad(train_data_preprocessed[\"time\"].dt.hour * 360 / 24)\n",
    "train_day = np.deg2rad(train_data_preprocessed[\"time\"].dt.day * 360 / 30)\n",
    "train_month = np.deg2rad(train_data_preprocessed[\"time\"].dt.month * 360 / 12)\n",
    "\n",
    "print(train_data_preprocessed[\"latitude\"].head(10))\n",
    "print(train_latitude_radians.head(10))\n",
    "\n",
    "\n",
    "train_latitude_sin = np.sin(train_latitude_radians)\n",
    "train_latitude_cos = np.cos(train_latitude_radians)\n",
    "\n",
    "train_longitude_sin = np.sin(train_longitude_radians)\n",
    "train_longitude_cos = np.cos(train_longitude_radians)\n",
    "\n",
    "train_cog_sin = np.sin(train_cog_radians)\n",
    "train_cog_cos = np.cos(train_cog_radians)\n",
    "\n",
    "train_heading_sin = np.sin(train_heading_radians)\n",
    "train_heading_cos = np.cos(train_heading_radians)\n",
    "\n",
    "train_hour_sin = np.sin(train_hour)\n",
    "train_hour_cos = np.cos(train_hour)\n",
    "\n",
    "train_day_sin = np.sin(train_day)\n",
    "train_day_cos = np.cos(train_day)\n",
    "\n",
    "train_month_sin = np.sin(train_month)\n",
    "train_month_cos = np.cos(train_month)\n",
    "\n",
    "\n",
    "train_data_preprocessed[\"latitude_sin\"] = train_latitude_sin\n",
    "train_data_preprocessed[\"latitude_cos\"] = train_latitude_cos\n",
    "train_data_preprocessed[\"longitude_sin\"] = train_longitude_sin\n",
    "train_data_preprocessed[\"longitude_cos\"] = train_longitude_cos\n",
    "train_data_preprocessed[\"cog_sin\"] = train_cog_sin\n",
    "train_data_preprocessed[\"cog_cos\"] = train_cog_cos\n",
    "train_data_preprocessed[\"heading_sin\"] = train_heading_sin\n",
    "train_data_preprocessed[\"heading_cos\"] = train_heading_cos\n",
    "\n",
    "train_data_preprocessed[\"hour_sin\"] = train_hour_sin\n",
    "train_data_preprocessed[\"hour_cos\"] = train_hour_cos\n",
    "train_data_preprocessed[\"day_sin\"] = train_day_sin\n",
    "train_data_preprocessed[\"day_cos\"] = train_day_cos\n",
    "train_data_preprocessed[\"month_sin\"] = train_month_sin\n",
    "train_data_preprocessed[\"month_cos\"] = train_month_cos\n",
    "\n",
    "\n",
    "train_data_preprocessed = train_data_preprocessed.drop(\n",
    "    columns=[\"latitude\", \"longitude\", \"cog\", \"heading\", \"portId\"], axis=1\n",
    ")\n",
    "print(train_data_preprocessed.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Last_known_location_training_data(data: pd.DataFrame, shift_lenghts) -> pd.DataFrame:\n",
    "    \"\"\"_summary_  Groups training data by vesselId, and propogates all data from last known location\n",
    "\n",
    "    Args:\n",
    "    data (_type_): _description_ the data to be altered\n",
    "\n",
    "    Returns:\n",
    "        _type_:? _description_ the altered data\n",
    "        \n",
    "    \"\"\"\n",
    "    all_test_data=pd.DataFrame()\n",
    "    \n",
    "    for shift_length in shift_lenghts:\n",
    "    \n",
    "        grouped_data = data.groupby(\"vesselId\").apply(lambda x: x.sort_values(\"time\"))\n",
    "\n",
    "        grouped_data[\"time_diff\"] = (\n",
    "            grouped_data[\"time\"].diff(-shift_length).dt.total_seconds().abs()\n",
    "        )\n",
    "\n",
    "        original_time_and_id = grouped_data[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"vesselId\",\n",
    "                \"latitude_sin\",\n",
    "                \"latitude_cos\",\n",
    "                \"longitude_sin\",\n",
    "                \"longitude_cos\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        shifted_data = grouped_data.shift(shift_length)\n",
    "        shifted_data[\n",
    "            [\n",
    "                \"last_latitude_sin\",\n",
    "                \"last_latitude_cos\",\n",
    "                \"last_longitude_sin\",\n",
    "                \"last_longitude_cos\",\n",
    "            ]\n",
    "        ] = shifted_data[[\"latitude_sin\", \"latitude_cos\", \"longitude_sin\", \"longitude_cos\"]]\n",
    "\n",
    "        shifted_data[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"vesselId\",\n",
    "                \"latitude_sin\",\n",
    "                \"latitude_cos\",\n",
    "                \"longitude_sin\",\n",
    "                \"longitude_cos\",\n",
    "            ]\n",
    "        ] = original_time_and_id[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"vesselId\",\n",
    "                \"latitude_sin\",\n",
    "                \"latitude_cos\",\n",
    "                \"longitude_sin\",\n",
    "                \"longitude_cos\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        # Drops all values with nan values\n",
    "        result = shifted_data.dropna().reset_index(drop=True)\n",
    "        \n",
    "        all_test_data = pd.concat([all_test_data, result], ignore_index=True)\n",
    "\n",
    "    \n",
    "    # Uncomment the line below if you want to remove the \"time\" column after processing\n",
    "    # data = data.drop(\"time\", axis=1)\n",
    "\n",
    "    return all_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time', 'sog', 'rot', 'navstat', 'vesselId', 'seconds_to_eta',\n",
      "       'latitude_sin', 'latitude_cos', 'longitude_sin', 'longitude_cos',\n",
      "       'cog_sin', 'cog_cos', 'heading_sin', 'heading_cos', 'hour_sin',\n",
      "       'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_25422/1200682903.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_data = data.groupby(\"vesselId\").apply(lambda x: x.sort_values(\"time\"))\n",
      "/var/tmp/ipykernel_25422/1200682903.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_data = data.groupby(\"vesselId\").apply(lambda x: x.sort_values(\"time\"))\n",
      "/var/tmp/ipykernel_25422/1200682903.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_data = data.groupby(\"vesselId\").apply(lambda x: x.sort_values(\"time\"))\n",
      "/var/tmp/ipykernel_25422/1200682903.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_data = data.groupby(\"vesselId\").apply(lambda x: x.sort_values(\"time\"))\n"
     ]
    }
   ],
   "source": [
    "print(train_data_preprocessed.columns)\n",
    "\n",
    "train_data_shifted_df = Last_known_location_training_data(train_data_preprocessed,[1,5,10,50])\n",
    "\n",
    "#train_data_shifted_df = train_data_shifted_df.drop(columns=[\"time\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_last_known_data_test(\n",
    "    test_data: pd.DataFrame, known_data: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"_summary_  Groups training data by vesselId, and propogates all data from last known location\n",
    "\n",
    "    Args:\n",
    "    data (_type_): _description_ the data to be altered\n",
    "\n",
    "    Returns:\n",
    "        _type_:? _description_ the altered data\n",
    "    \"\"\"\n",
    "\n",
    "    if not test_data[\"vesselId\"].isin(known_data[\"vesselId\"]).all():\n",
    "        missing_vessels = test_data[\n",
    "            ~test_data[\"vesselId\"].isin(known_data[\"vesselId\"])\n",
    "        ][\"vesselId\"].unique()\n",
    "        raise ValueError(\n",
    "            f\"The following vesselIds are missing in known_data: {missing_vessels}\"\n",
    "        )\n",
    "    print(\n",
    "        test_data[~test_data[\"vesselId\"].isin(known_data[\"vesselId\"])][\n",
    "            \"vesselId\"\n",
    "        ].unique()\n",
    "    )\n",
    "\n",
    "    grouped_data = (\n",
    "        known_data.sort_values(\"time\")\n",
    "        .groupby(\"vesselId\")\n",
    "        .tail(1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    original_time = test_data[[\"time\"]]\n",
    "    test_data = test_data.drop(\"time\", axis=1)\n",
    "\n",
    "    result = pd.merge(test_data, grouped_data, how=\"left\", on=\"vesselId\")\n",
    "\n",
    "    result[\"time_diff\"] = (original_time[\"time\"] - result[\"time\"]).dt.total_seconds()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "   ID                  vesselId  scaling_factor                time   sog  \\\n",
      "0   0  61e9f3aeb937134a3c4bfe3d             0.3 2024-05-07 23:48:16   0.0   \n",
      "1   1  61e9f473b937134a3c4c02df             0.3 2024-05-07 23:57:16   0.0   \n",
      "2   2  61e9f469b937134a3c4c029b             0.3 2024-05-07 23:59:08  18.7   \n",
      "3   3  61e9f45bb937134a3c4c0221             0.3 2024-05-07 23:52:34   0.1   \n",
      "4   4  61e9f38eb937134a3c4bfd8d             0.3 2024-05-07 23:51:29   0.3   \n",
      "\n",
      "   rot  navstat  seconds_to_eta   cog_sin   cog_cos  ...  hour_cos   day_sin  \\\n",
      "0  0.0        5       -133396.0  0.006981 -0.999976  ...  0.965926  0.994522   \n",
      "1  0.0        5       -521836.0  0.417867  0.908508  ...  0.965926  0.994522   \n",
      "2  0.0        0         45952.0  0.139173  0.990268  ...  0.965926  0.994522   \n",
      "3  0.0        1        -81454.0 -0.625243  0.780430  ...  0.965926  0.994522   \n",
      "4  0.0        2        101311.0 -0.933580  0.358368  ...  0.965926  0.994522   \n",
      "\n",
      "    day_cos  month_sin  month_cos  time_diff  last_latitude_sin  \\\n",
      "0  0.104528        0.5  -0.866025      900.0           0.517228   \n",
      "1  0.104528        0.5  -0.866025      541.0           0.255732   \n",
      "2  0.104528        0.5  -0.866025      654.0           0.619491   \n",
      "3  0.104528        0.5  -0.866025     1080.0          -0.688834   \n",
      "4  0.104528        0.5  -0.866025     1258.0           0.749340   \n",
      "\n",
      "   last_latitude_cos  last_longitude_sin  last_longitude_cos  \n",
      "0           0.855848           -0.989010            0.147846  \n",
      "1           0.966748            0.863429           -0.504471  \n",
      "2           0.785004            0.187086            0.982343  \n",
      "3           0.724919            0.124723           -0.992192  \n",
      "4           0.662186           -0.106612            0.994301  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "test_data_with_last_known_df = append_last_known_data_test(\n",
    "    test_data, train_data_preprocessed\n",
    ")\n",
    "test_data_with_last_known_df[\n",
    "    [\n",
    "        \"last_latitude_sin\",\n",
    "        \"last_latitude_cos\",\n",
    "        \"last_longitude_sin\",\n",
    "        \"last_longitude_cos\",\n",
    "    ]\n",
    "] = test_data_with_last_known_df[\n",
    "    [\n",
    "        \"latitude_sin\",\n",
    "        \"latitude_cos\",\n",
    "        \"longitude_sin\",\n",
    "        \"longitude_cos\",\n",
    "    ]\n",
    "]\n",
    "test_data_with_last_known_df = test_data_with_last_known_df.drop(\n",
    "    columns=[\n",
    "        \"latitude_sin\",\n",
    "        \"latitude_cos\",\n",
    "        \"longitude_sin\",\n",
    "        \"longitude_cos\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(test_data_with_last_known_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"11.0.24\" 2024-07-16; OpenJDK Runtime Environment (build 11.0.24+8-post-Debian-2deb11u1); OpenJDK 64-Bit Server VM (build 11.0.24+8-post-Debian-2deb11u1, mixed mode, sharing)\n",
      "  Starting server from /opt/conda/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/tmp/tmpkpvilywk\n",
      "  JVM stdout: /var/tmp/tmpkpvilywk/h2o_jupyter_started_from_python.out\n",
      "  JVM stderr: /var/tmp/tmpkpvilywk/h2o_jupyter_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.5</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>1 month and 22 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_jupyter_0l2a4b</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>24 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.15 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  ------------------------------\n",
       "H2O_cluster_uptime:         02 secs\n",
       "H2O_cluster_timezone:       Etc/UTC\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.5\n",
       "H2O_cluster_version_age:    1 month and 22 days\n",
       "H2O_cluster_name:           H2O_from_python_jupyter_0l2a4b\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    24 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.15 final\n",
       "--------------------------  ------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "h2o.init(max_mem_size=\"24g\")\n",
    "\n",
    "train_data_shifted = h2o.H2OFrame(train_data_shifted_df)\n",
    "test_data_with_last_known = h2o.H2OFrame(test_data_with_last_known_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_shifted_without_validation, validation_data_shifted = (\n",
    "    train_data_shifted.split_frame(ratios=[0.8], seed=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lat = [\n",
    "    #\"vesselId\",\n",
    "    \"cog_sin\",\n",
    "    \"cog_cos\",\n",
    "    \"sog\",\n",
    "    \"rot\",\n",
    "    \"heading_sin\",\n",
    "    \"heading_cos\",\n",
    "    \"navstat\",\n",
    "    \"time_diff\",\n",
    "    \"seconds_to_eta\",\n",
    "    \"last_latitude_sin\",\n",
    "    \"last_latitude_cos\",\n",
    "    \"last_longitude_sin\",\n",
    "    \"last_longitude_cos\",\n",
    "    \"hour_sin\",\n",
    "    \"hour_cos\",\n",
    "    \"day_sin\",\n",
    "    \"day_cos\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "]\n",
    "features_long = [\n",
    "    #\"vesselId\",\n",
    "    \"cog_sin\",\n",
    "    \"cog_cos\",\n",
    "    \"sog\",\n",
    "    \"rot\",\n",
    "    \"heading_sin\",\n",
    "    \"heading_cos\",\n",
    "    \"navstat\",\n",
    "    \"time_diff\",\n",
    "    \"seconds_to_eta\",\n",
    "    \"last_latitude_sin\",\n",
    "    \"last_latitude_cos\",\n",
    "    \"last_longitude_sin\",\n",
    "    \"last_longitude_cos\",\n",
    "    \"latitude_sin\",\n",
    "    \"latitude_cos\",\n",
    "    \"hour_sin\",\n",
    "    \"hour_cos\",\n",
    "    \"day_sin\",\n",
    "    \"day_cos\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "]\n",
    "target_long_sin = \"longitude_sin\"\n",
    "target_long_cos = \"longitude_cos\"\n",
    "target_lat_sin = \"latitude_sin\"\n",
    "target_lat_cos = \"latitude_cos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"ntrees\": 300,  # Maximum number of trees\n",
    "    \"max_depth\": 12,  # Maximum depth of each tree\n",
    "    \"min_rows\": 15,  # Minimum number of rows per leaf\n",
    "    \"learn_rate\": 0.1,  # Learning rate\n",
    "    \"sample_rate\": 0.8,  # Row sample rate per tree\n",
    "    \"col_sample_rate\": 0.7,  # Column sample rate per tree\n",
    "    \"reg_lambda\": 1.0,  # L2 regularization term\n",
    "    \"reg_alpha\": 0.1,  # L1 regularization term\n",
    "    \"seed\": 42,  # Random seed for reproducibility\n",
    "    \"stopping_rounds\": 10,  # Early stopping rounds\n",
    "    \"stopping_metric\": \"AUTO\",  # Metric for early stopping\n",
    "    \"stopping_tolerance\": 0.001  # Tolerance for early stopping\n",
    "}\n",
    "#score_eval_metric_only if you want to predict only on the evaluation dataset, could help against overfitting\n",
    "\n",
    "gbm_lat_sin = h2o.estimators.H2OXGBoostEstimator(\n",
    " **params\n",
    ")\n",
    "gbm_lat_cos = h2o.estimators.H2OXGBoostEstimator(\n",
    "**params\n",
    ")\n",
    "gbm_long_sin = h2o.estimators.H2OXGBoostEstimator(\n",
    "**params\n",
    ")\n",
    "gbm_long_cos = h2o.estimators.H2OXGBoostEstimator(\n",
    "**params\n",
    ")\n",
    "\n",
    "# gbm_cog = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_sog = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_rot = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_heading = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_navstat = h2o.estimators.H2OXGBoostEstimator()\n",
    "# # gbm_etaRaw = h2o.esti#mators.H2OXGBoostEstimator() #Remove etaRaw because it requires preprocessing\n",
    "# # gbm_portId = h2o.estimators.H2OXGBoostEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:192: RuntimeWarning: early stopping is enabled but neither score_tree_interval or score_each_iteration are defined. Early stopping will not be reproducible!\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "████████"
     ]
    }
   ],
   "source": [
    "gbm_lat_sin.train(\n",
    "    x=features_lat,\n",
    "    y=target_lat_sin,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    "    validation_frame= validation_data_shifted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lat.append(\"latitude_sin\")\n",
    "gbm_lat_cos.train(\n",
    "    x=features_lat,#.append(latitude_sin)\n",
    "    y=target_lat_cos,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    "    validation_frame= validation_data_shifted\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_lat_sin = gbm_lat_sin.model_performance(test_data=validation_data_shifted)\n",
    "performance_lat_cos = gbm_lat_cos.model_performance(test_data=validation_data_shifted)\n",
    "\n",
    "\n",
    "# Print the performance metrics\n",
    "print(performance_lat_sin)\n",
    "print(performance_lat_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_long_sin.train(\n",
    "    x=features_long,\n",
    "    y=target_long_sin,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    "    validation_frame= validation_data_shifted\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_long.append(\"longitude_sin\")\n",
    "gbm_long_cos.train(\n",
    "    x=features_long,#.append(\"longitude_sin\")\n",
    "    y=target_long_cos,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    "    validation_frame= validation_data_shifted\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_long_sin = gbm_long_sin.model_performance(test_data=validation_data_shifted)\n",
    "performance_long_cos = gbm_long_cos.model_performance(test_data=validation_data_shifted)\n",
    "\n",
    "print(performance_long_sin)\n",
    "print(performance_long_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_predicted_lat = test_data_with_last_known\n",
    "\n",
    "lat_predictions_sin = gbm_lat_sin.predict(test_data_with_last_known)\n",
    "test_data_with_last_known[\"latitude_sin\"]=lat_predictions_sin\n",
    "lat_predictions_cos = gbm_lat_cos.predict(test_data_with_last_known)\n",
    "test_data_with_predicted_lat[\"latitude_cos\"] = lat_predictions_cos\n",
    "\n",
    "test_data_with_predicted_lat[\"latitude_sin\"] = lat_predictions_sin\n",
    "test_data_with_predicted_lat[\"latitude_cos\"] = lat_predictions_cos\n",
    "\n",
    "long_predictions_sin = gbm_long_sin.predict(test_data_with_predicted_lat)\n",
    "test_data_with_last_known[\"longitude_sin\"]=long_predictions_sin\n",
    "long_predictions_cos = gbm_long_cos.predict(test_data_with_predicted_lat)\n",
    "test_data_with_last_known[\"longitude_cos\"]=long_predictions_cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sine and cosine values back to radians\n",
    "lat_predictions_sin = lat_predictions_sin.as_data_frame()\n",
    "lat_predictions_cos = lat_predictions_cos.as_data_frame()\n",
    "long_predictions_sin = long_predictions_sin.as_data_frame()\n",
    "long_predictions_cos = long_predictions_cos.as_data_frame()\n",
    "\n",
    "\n",
    "lat_predictions_radians = np.arctan2(lat_predictions_sin, lat_predictions_cos)\n",
    "long_predictions_radians = np.arctan2(long_predictions_sin, long_predictions_cos)\n",
    "\n",
    "# Convert radians to degrees\n",
    "lat_predictions_degrees = np.rad2deg(lat_predictions_radians)\n",
    "long_predictions_degrees = np.rad2deg(long_predictions_radians)\n",
    "\n",
    "# Print the first few rows to verify the conversion\n",
    "print(lat_predictions_degrees.head())\n",
    "print(long_predictions_degrees.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prediction_visualization_data(validation_data):\n",
    "    lat_val_sin = gbm_lat_sin.predict(validation_data)\n",
    "    lat_val_cos = gbm_lat_cos.predict(validation_data)\n",
    "    long_val_sin = gbm_long_sin.predict(validation_data)\n",
    "    long_val_cos = gbm_long_cos.predict(validation_data)\n",
    "\n",
    "    lat_val_sin = lat_val_sin.as_data_frame()\n",
    "    lat_val_cos = lat_val_cos.as_data_frame()\n",
    "    long_val_sin = long_val_sin.as_data_frame()\n",
    "    long_val_cos = long_val_cos.as_data_frame()\n",
    "\n",
    "    validation_data=validation_data.as_data_frame()\n",
    "\n",
    "    lat_val_radians = np.arctan2(lat_val_sin, lat_val_cos)\n",
    "    long_val_radians = np.arctan2(long_val_sin, long_val_cos)\n",
    "\n",
    "    evaluation_lat_radians=np.arctan2(validation_data[\"latitude_sin\"], validation_data[\"latitude_cos\"])\n",
    "    evaluation_long_radians=np.arctan2(validation_data[\"longitude_sin\"], validation_data[\"longitude_cos\"])\n",
    "\n",
    "\n",
    "    # Convert radians to degrees\n",
    "    lat_val_degrees = np.rad2deg(lat_val_radians)\n",
    "    long_val_degrees = np.rad2deg(long_val_radians)\n",
    "\n",
    "    evaluation_lat_degrees=np.rad2deg(evaluation_lat_radians)\n",
    "    evaluation_long_degrees=np.rad2deg(evaluation_long_radians)\n",
    "\n",
    "\n",
    "    eval_predictions = pd.concat([lat_val_degrees, long_val_degrees], axis=1)\n",
    "\n",
    "    eval_actual= pd.concat([evaluation_lat_degrees,evaluation_long_degrees], axis=1)\n",
    "\n",
    "    eval_predictions.columns = [\"latitude_predicted\", \"longitude_predicted\"]\n",
    "    eval_actual.columns = [\"latitude\", \"longitude\"]\n",
    "\n",
    "    \n",
    "    eval = pd.DataFrame()\n",
    "    eval[[\"latitude_predicted\",\"longitude_predicted\"]] = eval_predictions\n",
    "    eval[[\"latitude\", \"longitude\"]] = eval_actual\n",
    "    eval[[\"vesselId\",\"time\"]] =validation_data[[\"vesselId\",\"time\"]]\n",
    "    eval.to_csv(\"eval_predictions.csv\")\n",
    "\n",
    "#create_prediction_visualization_data(validation_data_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat([lat_predictions_degrees, long_predictions_degrees], axis=1)\n",
    "predictions.columns = [\"latitude_predicted\", \"longitude_predicted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"ID\"] = test_data[\"ID\"]\n",
    "predictions = predictions[[\"ID\", \"longitude_predicted\", \"latitude_predicted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
