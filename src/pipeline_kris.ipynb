{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import h2o\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "# from .feature_engineering_filter import Find_correct_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hy3nmqqx6f70nkbvw0n8lbh80000gn/T/ipykernel_94604/3287064515.py:4: H2ODeprecationWarning: Deprecated, use ``h2o.cluster().shutdown()``.\n",
      "  h2o.shutdown(prompt=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An H2O cluster is already running. Shutting it down...\n",
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"17.0.1\" 2021-10-19; OpenJDK Runtime Environment Temurin-17.0.1+12 (build 17.0.1+12); OpenJDK 64-Bit Server VM Temurin-17.0.1+12 (build 17.0.1+12, mixed mode, sharing)\n",
      "  Starting server from /Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/3t/hy3nmqqx6f70nkbvw0n8lbh80000gn/T/tmpoqt649v2\n",
      "  JVM stdout: /var/folders/3t/hy3nmqqx6f70nkbvw0n8lbh80000gn/T/tmpoqt649v2/h2o_kristofferseyffarth_started_from_python.out\n",
      "  JVM stderr: /var/folders/3t/hy3nmqqx6f70nkbvw0n8lbh80000gn/T/tmpoqt649v2/h2o_kristofferseyffarth_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54341\n",
      "Connecting to H2O server at http://127.0.0.1:54341 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-3.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-3 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-3 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-3 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-3 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-3 .h2o-table th,\n",
       "#h2o-table-3 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-3 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-3\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>05 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Oslo</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.5</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>1 month and 21 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_kristofferseyffarth_bc70ad</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>4 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54341</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.12.5 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------\n",
       "H2O_cluster_uptime:         05 secs\n",
       "H2O_cluster_timezone:       Europe/Oslo\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.5\n",
       "H2O_cluster_version_age:    1 month and 21 days\n",
       "H2O_cluster_name:           H2O_from_python_kristofferseyffarth_bc70ad\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    4 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54341\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.12.5 final\n",
       "--------------------------  ------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if an H2O cluster is already running\n",
    "if h2o.cluster().is_running():\n",
    "    print(\"An H2O cluster is already running. Shutting it down...\")\n",
    "    h2o.shutdown(prompt=True)\n",
    "\n",
    "h2o.init(max_mem_size=\"4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../Datasets/ais_train.csv\", delimiter=\"|\")\n",
    "test_data = pd.read_csv(\"../Datasets/ais_test.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"time\"] = pd.to_datetime(train_data[\"time\"])\n",
    "test_data[\"time\"] = pd.to_datetime(test_data[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    01-09 23:00\n",
      "1    12-29 20:00\n",
      "2            NaN\n",
      "3    12-31 20:00\n",
      "4            NaN\n",
      "Name: etaRaw, dtype: object\n",
      "0    01-09 23:00\n",
      "1    12-29 20:00\n",
      "2            NaN\n",
      "3    12-31 20:00\n",
      "4            NaN\n",
      "Name: etaRaw, dtype: object\n",
      "0    01-09 23:00\n",
      "1    12-29 20:00\n",
      "2            NaN\n",
      "3    12-31 20:00\n",
      "4            NaN\n",
      "Name: etaRaw, dtype: object\n",
      "0    01-09 23:00\n",
      "1    12-29 20:00\n",
      "3    12-31 20:00\n",
      "5    12-20 02:40\n",
      "7    12-31 18:30\n",
      "Name: etaRaw, dtype: object\n",
      "0   2024-01-09 23:00:00\n",
      "1   2024-12-29 20:00:00\n",
      "3   2024-12-31 20:00:00\n",
      "5   2024-12-20 02:40:00\n",
      "7   2024-12-31 18:30:00\n",
      "Name: etaRaw, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "train_data_preprocessed = train_data.copy()\n",
    "train_data_preprocessed[train_data_preprocessed[\"cog\"] >= 360] = np.nan\n",
    "train_data_preprocessed[train_data_preprocessed[\"sog\"] >= 1.023] = np.nan\n",
    "train_data_preprocessed[train_data_preprocessed[\"rot\"] == -128] = np.nan\n",
    "train_data_preprocessed[train_data_preprocessed[\"heading\"] == 511] = np.nan\n",
    "\n",
    "train_data_preprocessed[\"cog\"] = (\n",
    "    train_data_preprocessed[\"cog\"].groupby(train_data_preprocessed[\"vesselId\"]).ffill()\n",
    ")\n",
    "train_data_preprocessed[\"sog\"] = (\n",
    "    train_data_preprocessed[\"sog\"].groupby(train_data_preprocessed[\"vesselId\"]).ffill()\n",
    ")\n",
    "train_data_preprocessed[\"rot\"] = (\n",
    "    train_data_preprocessed[\"rot\"].groupby(train_data_preprocessed[\"vesselId\"]).ffill()\n",
    ")\n",
    "train_data_preprocessed[\"heading\"] = (\n",
    "    train_data_preprocessed[\"heading\"]\n",
    "    .groupby(train_data_preprocessed[\"vesselId\"])\n",
    "    .ffill()\n",
    ")\n",
    "print(train_data_preprocessed[\"etaRaw\"].head())\n",
    "\n",
    "pattern = r\"^\\d{2}-\\d{2} \\d{2}:\\d{2}$\"\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].where(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.match(pattern, na=False), np.nan\n",
    ")\n",
    "print(train_data_preprocessed[\"etaRaw\"].head())\n",
    "\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed.groupby(\"vesselId\")[\n",
    "    \"etaRaw\"\n",
    "].ffill()\n",
    "\n",
    "print(train_data_preprocessed[\"etaRaw\"].head())\n",
    "\n",
    "\n",
    "train_data_preprocessed[\"navstat\"] = train_data_preprocessed[\"navstat\"].astype(\n",
    "    \"category\"\n",
    ")\n",
    "\n",
    "train_data_preprocessed = train_data_preprocessed.dropna(subset=[\"etaRaw\"])\n",
    "\n",
    "print(train_data_preprocessed[\"etaRaw\"].head())\n",
    "\n",
    "# # Replace '00-' in etaRaw with the corresponding month and day from the 'time' column\n",
    "# train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "#     train_data_preprocessed[\"etaRaw\"].str.contains(\"00-\", na=False),\n",
    "#     train_data_preprocessed[\"time\"].dt.strftime(\n",
    "#     \"%m-%d\"\n",
    "# )\n",
    "#     + \" \"\n",
    "#     + train_data_preprocessed[\"etaRaw\"].str[6:],\n",
    "# )\n",
    "\n",
    "# Replace '00-' in etaRaw with the corresponding month and day from the 'time' column\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"00-\", na=False),\n",
    "    \"01\"\n",
    "    + train_data_preprocessed[\"etaRaw\"].str[2:],\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"-00\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:2]\n",
    "    +\"-01\"\n",
    "    + train_data_preprocessed[\"etaRaw\"].str[5:],\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\":60\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:9]+\"59\",\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"60:\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:6]+\"01:00\",\n",
    ")\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = train_data_preprocessed[\"etaRaw\"].mask(\n",
    "    train_data_preprocessed[\"etaRaw\"].str.contains(\"24:\", na=False),\n",
    "    train_data_preprocessed[\"etaRaw\"].str[:6]+\"23:59\"\n",
    ")\n",
    "\n",
    "\n",
    "train_data_preprocessed[\"etaRaw\"] = pd.to_datetime(\n",
    "    train_data_preprocessed[\"time\"].dt.year.astype(str)\n",
    "    + \"-\"\n",
    "    + train_data_preprocessed[\"etaRaw\"]\n",
    "    + \":00\",\n",
    "    format=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "\n",
    "print(train_data_preprocessed[\"etaRaw\"].head())\n",
    "train_data_preprocessed[\"seconds_to_eta\"] = (\n",
    "    train_data_preprocessed[\"time\"] - train_data_preprocessed[\"etaRaw\"]\n",
    ").dt.total_seconds()\n",
    "\n",
    "train_data_preprocessed = train_data_preprocessed.drop(columns=[\"etaRaw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time   sog  rot navstat       etaRaw  \\\n",
      "0 2024-01-01 00:00:25   0.7    0       0  01-09 23:00   \n",
      "1 2024-01-01 00:00:36   0.0   -6       1  12-29 20:00   \n",
      "2 2024-01-01 00:01:45  11.0    0       0  01-02 09:00   \n",
      "3 2024-01-01 00:03:11   0.0    0       1  12-31 20:00   \n",
      "4 2024-01-01 00:03:51  19.7    0       0  01-25 12:00   \n",
      "\n",
      "                   vesselId  latitude_sin  latitude_cos  longitude_sin  \\\n",
      "0  61e9f3a8b937134a3c4bfdf7     -0.569906      0.821710      -0.846670   \n",
      "1  61e9f3d4b937134a3c4bff1f      0.154614      0.987975      -0.983189   \n",
      "2  61e9f436b937134a3c4c0131      0.631903      0.775048      -0.972271   \n",
      "3  61e9f3b4b937134a3c4bfe77     -0.565138      0.824996       0.484494   \n",
      "4  61e9f41bb937134a3c4c0087      0.586143      0.810208      -0.103077   \n",
      "\n",
      "   longitude_cos   cog_sin   cog_cos  heading_sin  heading_cos  \n",
      "0       0.532118 -0.846670  0.532118    -0.846670     0.532118  \n",
      "1       0.182589 -0.983189  0.182589    -0.983189     0.182589  \n",
      "2       0.233858 -0.972271  0.233858    -0.972271     0.233858  \n",
      "3      -0.874795  0.484494 -0.874795     0.484494    -0.874795  \n",
      "4       0.994673 -0.103077  0.994673    -0.103077     0.994673  \n"
     ]
    }
   ],
   "source": [
    "train_latitude_radians = np.deg2rad(train_data[\"latitude\"])\n",
    "train_longitude_radians = np.deg2rad(train_data[\"longitude\"])\n",
    "train_cog_radians = np.deg2rad(train_data[\"longitude\"])\n",
    "train_heading_radians = np.deg2rad(train_data[\"longitude\"])\n",
    "train_hour = np.deg2rad(train_data[\"time\"].dt.hour * 360 / 24)\n",
    "train_day = np.deg2rad(train_data[\"time\"].dt.day * 360 / 30)\n",
    "train_month = np.deg2rad(train_data[\"time\"].dt.month * 360 / 12)\n",
    "\n",
    "train_latitude_sin = np.sin(train_latitude_radians)\n",
    "train_latitude_cos = np.cos(train_latitude_radians)\n",
    "\n",
    "train_longitude_sin = np.sin(train_longitude_radians)\n",
    "train_longitude_cos = np.cos(train_longitude_radians)\n",
    "\n",
    "train_cog_sin = np.sin(train_cog_radians)\n",
    "train_cog_cos = np.cos(train_cog_radians)\n",
    "\n",
    "train_heading_sin = np.sin(train_heading_radians)\n",
    "train_heading_cos = np.cos(train_heading_radians)\n",
    "\n",
    "train_hour_sin = np.sin(train_hour)\n",
    "train_hour_cos = np.cos(train_hour)\n",
    "\n",
    "train_day_sin = np.sin(train_day)\n",
    "train_day_cos = np.cos(train_day)\n",
    "\n",
    "train_month_sin = np.sin(train_month)\n",
    "train_month_cos = np.cos(train_month)\n",
    "\n",
    "\n",
    "train_data_preprocessed[\"latitude_sin\"] = train_latitude_sin\n",
    "train_data_preprocessed[\"latitude_cos\"] = train_latitude_cos\n",
    "train_data_preprocessed[\"longitude_sin\"] = train_longitude_sin\n",
    "train_data_preprocessed[\"longitude_cos\"] = train_longitude_cos\n",
    "train_data_preprocessed[\"cog_sin\"] = train_cog_sin\n",
    "train_data_preprocessed[\"cog_cos\"] = train_cog_cos\n",
    "train_data_preprocessed[\"heading_sin\"] = train_heading_sin\n",
    "train_data_preprocessed[\"heading_cos\"] = train_heading_cos\n",
    "\n",
    "train_data_preprocessed[\"hour_sin\"] = train_hour_sin\n",
    "train_data_preprocessed[\"hour_cos\"] = train_hour_cos\n",
    "train_data_preprocessed[\"day_sin\"] = train_day_sin\n",
    "train_data_preprocessed[\"day_cos\"] = train_day_cos\n",
    "train_data_preprocessed[\"month_sin\"] = train_month_sin\n",
    "train_data_preprocessed[\"month_cos\"] = train_month_cos\n",
    "\n",
    "\n",
    "train_data_preprocessed = train_data_preprocessed.drop(\n",
    "    columns=[\"latitude\", \"longitude\", \"cog\", \"heading\", \"portId\"], axis=1\n",
    ")\n",
    "print(train_data_preprocessed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Last_known_location_training_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"_summary_  Groups training data by vesselId, and propogates all data from last known location\n",
    "\n",
    "    Args:\n",
    "    data (_type_): _description_ the data to be altered\n",
    "\n",
    "    Returns:\n",
    "        _type_:? _description_ the altered data\n",
    "    \"\"\"\n",
    "\n",
    "    shift_length = 10\n",
    "\n",
    "    grouped_data = data.groupby(\"vesselId\").apply(lambda x: x.sort_values(\"time\"))\n",
    "\n",
    "    grouped_data[\"time_diff\"] = (\n",
    "        grouped_data[\"time\"].diff(-shift_length).dt.total_seconds().abs().fillna(0)\n",
    "    )\n",
    "\n",
    "    original_time_and_id = grouped_data[\n",
    "        [\n",
    "            \"time\",\n",
    "            \"vesselId\",\n",
    "            \"latitude_sin\",\n",
    "            \"latitude_cos\",\n",
    "            \"longitude_sin\",\n",
    "            \"longitude_cos\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    shifted_data = grouped_data.shift(shift_length)\n",
    "    shifted_data[\n",
    "        [\n",
    "            \"last_latitude_sin\",\n",
    "            \"last_latitude_cos\",\n",
    "            \"last_longitude_sin\",\n",
    "            \"last_longitude_cos\",\n",
    "        ]\n",
    "    ] = shifted_data[[\"latitude_sin\", \"latitude_cos\", \"longitude_sin\", \"longitude_cos\"]]\n",
    "\n",
    "    shifted_data[\n",
    "        [\n",
    "            \"time\",\n",
    "            \"vesselId\",\n",
    "            \"latitude_sin\",\n",
    "            \"latitude_cos\",\n",
    "            \"longitude_sin\",\n",
    "            \"longitude_cos\",\n",
    "        ]\n",
    "    ] = original_time_and_id[\n",
    "        [\n",
    "            \"time\",\n",
    "            \"vesselId\",\n",
    "            \"latitude_sin\",\n",
    "            \"latitude_cos\",\n",
    "            \"longitude_sin\",\n",
    "            \"longitude_cos\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Drops all values with nan values\n",
    "    result = shifted_data.dropna().reset_index(drop=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hy3nmqqx6f70nkbvw0n8lbh80000gn/T/ipykernel_94604/824791856.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_data = data.groupby(\"vesselId\").apply(lambda x: x.sort_values(\"time\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "train_data_shifted_df = Last_known_location_training_data(train_data_preprocessed)\n",
    "\n",
    "train_data_shifted_df = train_data_shifted_df.drop([\"time\", \"etaRaw\"], axis=1)\n",
    "\n",
    "train_data_shifted = h2o.H2OFrame(train_data_shifted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_shifted_without_validation, validation_data_shifted = (\n",
    "    train_data_shifted.split_frame(ratios=[0.8], seed=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_last_known_data_test(\n",
    "    test_data: pd.DataFrame, known_data: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"_summary_  Groups training data by vesselId, and propogates all data from last known location\n",
    "\n",
    "    Args:\n",
    "    data (_type_): _description_ the data to be altered\n",
    "\n",
    "    Returns:\n",
    "        _type_:? _description_ the altered data\n",
    "    \"\"\"\n",
    "\n",
    "    if not test_data[\"vesselId\"].isin(known_data[\"vesselId\"]).all():\n",
    "        missing_vessels = test_data[\n",
    "            ~test_data[\"vesselId\"].isin(known_data[\"vesselId\"])\n",
    "        ][\"vesselId\"].unique()\n",
    "        raise ValueError(\n",
    "            f\"The following vesselIds are missing in known_data: {missing_vessels}\"\n",
    "        )\n",
    "    print(\n",
    "        test_data[~test_data[\"vesselId\"].isin(known_data[\"vesselId\"])][\n",
    "            \"vesselId\"\n",
    "        ].unique()\n",
    "    )\n",
    "\n",
    "    grouped_data = (\n",
    "        known_data.sort_values(\"time\")\n",
    "        .groupby(\"vesselId\")\n",
    "        .tail(1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    original_time = test_data[[\"time\"]]\n",
    "    test_data = test_data.drop(\"time\", axis=1)\n",
    "\n",
    "    result = pd.merge(test_data, grouped_data, how=\"left\", on=\"vesselId\")\n",
    "\n",
    "    result[\"time_diff\"] = (original_time[\"time\"] - result[\"time\"]).dt.total_seconds()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "   ID                  vesselId  scaling_factor                time   sog  \\\n",
      "0   0  61e9f3aeb937134a3c4bfe3d             0.3 2024-05-07 23:48:16   0.0   \n",
      "1   1  61e9f473b937134a3c4c02df             0.3 2024-05-07 23:57:16   0.0   \n",
      "2   2  61e9f469b937134a3c4c029b             0.3 2024-05-07 23:59:08  18.7   \n",
      "3   3  61e9f45bb937134a3c4c0221             0.3 2024-05-07 23:52:34   0.1   \n",
      "4   4  61e9f38eb937134a3c4bfd8d             0.3 2024-05-07 23:51:29   0.3   \n",
      "\n",
      "   rot navstat       etaRaw   cog_sin   cog_cos  heading_sin  heading_cos  \\\n",
      "0    0       5  05-06 10:45 -0.989010  0.147846    -0.989010     0.147846   \n",
      "1    0       5  05-01 23:00  0.863429 -0.504471     0.863429    -0.504471   \n",
      "2    0       0  05-08 12:45  0.187086  0.982343     0.187086     0.982343   \n",
      "3    0       1  05-07 01:15  0.124723 -0.992192     0.124723    -0.992192   \n",
      "4    0       2  05-09 04:00 -0.106612  0.994301    -0.106612     0.994301   \n",
      "\n",
      "   time_diff  last_latitude_sin  last_latitude_cos  last_longitude_sin  \\\n",
      "0      900.0           0.517228           0.855848           -0.989010   \n",
      "1      541.0           0.255732           0.966748            0.863429   \n",
      "2      654.0           0.619491           0.785004            0.187086   \n",
      "3     1080.0          -0.688834           0.724919            0.124723   \n",
      "4     1258.0           0.749340           0.662186           -0.106612   \n",
      "\n",
      "   last_longitude_cos  \n",
      "0            0.147846  \n",
      "1           -0.504471  \n",
      "2            0.982343  \n",
      "3           -0.992192  \n",
      "4            0.994301  \n"
     ]
    }
   ],
   "source": [
    "test_data_with_last_known_df = append_last_known_data_test(\n",
    "    test_data, train_data_preprocessed\n",
    ")\n",
    "test_data_with_last_known_df[\n",
    "    [\n",
    "        \"last_latitude_sin\",\n",
    "        \"last_latitude_cos\",\n",
    "        \"last_longitude_sin\",\n",
    "        \"last_longitude_cos\",\n",
    "    ]\n",
    "] = test_data_with_last_known_df[\n",
    "    [\n",
    "        \"latitude_sin\",\n",
    "        \"latitude_cos\",\n",
    "        \"longitude_sin\",\n",
    "        \"longitude_cos\",\n",
    "    ]\n",
    "]\n",
    "test_data_with_last_known_df = test_data_with_last_known_df.drop(\n",
    "    columns=[\n",
    "        \"latitude_sin\",\n",
    "        \"latitude_cos\",\n",
    "        \"longitude_sin\",\n",
    "        \"longitude_cos\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "test_data_with_last_known = h2o.H2OFrame(test_data_with_last_known_df)\n",
    "print(test_data_with_last_known_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_last_known_df.to_csv(\"../Datasets/test_data_with_last_known.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lat = [\n",
    "    \"vesselId\",\n",
    "    \"cog_sin\",\n",
    "    \"cog_cos\",\n",
    "    \"sog\",\n",
    "    \"rot\",\n",
    "    \"heading_sin\",\n",
    "    \"heading_cos\",\n",
    "    \"navstat\",\n",
    "    \"time_diff\",\n",
    "    \"last_latitude_sin\",\n",
    "    \"last_latitude_cos\",\n",
    "    \"last_longitude_sin\",\n",
    "    \"last_longitude_cos\",\n",
    "]\n",
    "features_long = [\n",
    "    \"vesselId\",\n",
    "    \"cog_sin\",\n",
    "    \"cog_cos\",\n",
    "    \"sog\",\n",
    "    \"rot\",\n",
    "    \"heading_sin\",\n",
    "    \"heading_cos\",\n",
    "    \"navstat\",\n",
    "    \"time_diff\",\n",
    "    \"last_latitude_sin\",\n",
    "    \"last_latitude_cos\",\n",
    "    \"last_longitude_sin\",\n",
    "    \"last_longitude_cos\",\n",
    "    \"latitude_sin\",\n",
    "    \"latitude_cos\",\n",
    "]\n",
    "target_long_sin = \"longitude_sin\"\n",
    "target_long_cos = \"longitude_cos\"\n",
    "target_lat_sin = \"latitude_sin\"\n",
    "target_lat_cos = \"latitude_cos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"ntrees\": 300,  # Maximum number of trees\n",
    "    \"max_depth\": 6,  # Reduced maximum depth of each tree\n",
    "    \"min_rows\": 20,  # Increased minimum number of rows per leaf\n",
    "    \"learn_rate\": 0.05,  # Learning rate\n",
    "    \"sample_rate\": 0.8,  # Reduced row sample rate per tree\n",
    "    \"col_sample_rate\": 0.8,  # Reduced column sample rate per tree\n",
    "    \"reg_lambda\": 1.0,  # L2 regularization term\n",
    "    \"reg_alpha\": 0.1,  # L1 regularization term\n",
    "    \"seed\": 42,  # Random seed for reproducibility\n",
    "    \"stopping_rounds\": 10,  # Early stopping rounds\n",
    "    \"stopping_metric\": \"AUTO\",  # Metric for early stopping\n",
    "    \"stopping_tolerance\": 0.001,  # Tolerance for early stopping\n",
    "}\n",
    "\n",
    "\n",
    "gbm_lat_sin = h2o.estimators.H2OXGBoostEstimator(\n",
    "    **params\n",
    "    # ntrees=200,  # Maximum number of trees\n",
    "    # max_depth=10,  # Maximum depth of each tree\n",
    "    # min_rows=10,  # Minimum number of rows per leaf\n",
    "    # learn_rate=0.05,  # Learning rate\n",
    "    # sample_rate=0.9,  # Row sample rate per tree\n",
    "    # col_sample_rate=0.9,  # Column sample rate per tree\n",
    "    # reg_lambda=1.0,  # L2 regularization term\n",
    "    # reg_alpha=0.1,  # L1 regularization term\n",
    "    # seed=42,  # Random seed for reproducibility\n",
    ")\n",
    "gbm_lat_cos = h2o.estimators.H2OXGBoostEstimator(\n",
    "    **params\n",
    "    # ntrees=200,  # Maximum number of trees\n",
    "    # max_depth=10,  # Maximum depth of each tree\n",
    "    # min_rows=10,  # Minimum number of rows per leaf\n",
    "    # learn_rate=0.05,  # Learning rate\n",
    "    # sample_rate=0.9,  # Row sample rate per tree\n",
    "    # col_sample_rate=0.9,  # Column sample rate per tree\n",
    "    # reg_lambda=1.0,  # L2 regularization term\n",
    "    # reg_alpha=0.1,  # L1 regularization term\n",
    "    # seed=42,  # Random seed for reproducibility\n",
    ")\n",
    "gbm_long_sin = h2o.estimators.H2OXGBoostEstimator(\n",
    "    **params\n",
    "    # ntrees=200,  # Maximum number of trees\n",
    "    # max_depth=10,  # Maximum depth of each tree\n",
    "    # min_rows=10,  # Minimum number of rows per leaf\n",
    "    # learn_rate=0.05,  # Learning rate\n",
    "    # sample_rate=0.9,  # Row sample rate per tree\n",
    "    # col_sample_rate=0.9,  # Column sample rate per tree\n",
    "    # reg_lambda=1.0,  # L2 regularization term\n",
    "    # reg_alpha=0.1,  # L1 regularization term\n",
    "    # seed=42,  # Random seed for reproducibility\n",
    ")\n",
    "gbm_long_cos = h2o.estimators.H2OXGBoostEstimator(\n",
    "    **params\n",
    "    # ntrees=200,  # Maximum number of trees\n",
    "    # max_depth=10,  # Maximum depth of each tree\n",
    "    # min_rows=10,  # Minimum number of rows per leaf\n",
    "    # learn_rate=0.05,  # Learning rate\n",
    "    # sample_rate=0.9,  # Row sample rate per tree\n",
    "    # col_sample_rate=0.9,  # Column sample rate per tree\n",
    "    # reg_lambda=1.0,  # L2 regularization term\n",
    "    # reg_alpha=0.1,  # L1 regularization term\n",
    "    # seed=42,  # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# gbm_cog = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_sog = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_rot = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_heading = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_navstat = h2o.estimators.H2OXGBoostEstimator()\n",
    "# # gbm_etaRaw = h2o.esti#mators.H2OXGBoostEstimator() #Remove etaRaw because it requires preprocessing\n",
    "# # gbm_portId = h2o.estimators.H2OXGBoostEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sog    rot    navstat  vesselId                    latitude_sin    latitude_cos    longitude_sin    longitude_cos    cog_sin    cog_cos    heading_sin    heading_cos        time_diff    last_latitude_sin    last_latitude_cos    last_longitude_sin    last_longitude_cos\n",
      " 17.1     -6          0  61e9f38eb937134a3c4bfd8b        0.140983        0.990012         0.973701         0.227828   0.97661    0.215018       0.97661        0.215018  12529                       0.130589             0.991437              0.97661               0.215018\n",
      " 17.3      5          0  61e9f38eb937134a3c4bfd8b        0.141929        0.989877         0.973407         0.229084   0.976277   0.216524       0.976277       0.216524  12396                       0.13179              0.991278              0.976277              0.216524\n",
      " 16.9      5          0  61e9f38eb937134a3c4bfd8b        0.142588        0.989782         0.973112         0.230331   0.975894   0.218245       0.975894       0.218245  11929                       0.133129             0.991099              0.975894              0.218245\n",
      " 16.9      6          0  61e9f38eb937134a3c4bfd8b        0.145708        0.989328         0.9722           0.234154   0.975588   0.219609       0.975588       0.219609  14412                       0.134207             0.990953              0.975588              0.219609\n",
      " 16.3      7          0  61e9f38eb937134a3c4bfd8b       -0.495102        0.868835         0.520885         0.853627   0.975288   0.220938       0.975288       0.220938      1.16869e+06             0.13523              0.990814              0.975288              0.220938\n",
      " 16.1      5          0  61e9f38eb937134a3c4bfd8b       -0.496009        0.868317         0.519907         0.854223   0.975078   0.221864       0.975078       0.221864      1.16905e+06             0.135938             0.990717              0.975078              0.221864\n",
      " 16.1     -6          0  61e9f38eb937134a3c4bfd8b       -0.496653        0.867949         0.519036         0.854752   0.974801   0.223075       0.974801       0.223075      1.1691e+06              0.136914             0.990583              0.974801              0.223075\n",
      " 16        2          0  61e9f38eb937134a3c4bfd8b       -0.496955        0.867776         0.518429         0.85512    0.974519   0.224303       0.974519       0.224303      1.16877e+06             0.137891             0.990447              0.974519              0.224303\n",
      " 16       -1          0  61e9f38eb937134a3c4bfd8b       -0.497146        0.867667         0.51772          0.85555    0.974247   0.225482       0.974247       0.225482      1.16879e+06             0.138872             0.99031               0.974247              0.225482\n",
      " 16.1      6          0  61e9f38eb937134a3c4bfd8b       -0.497257        0.867603         0.517543         0.855657   0.97398    0.226635       0.97398        0.226635      1.16885e+06             0.139795             0.99018               0.97398               0.226635\n",
      "[10 rows x 17 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data_shifted_without_validation.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/estimators/estimator_base.py:192: RuntimeWarning: early stopping is enabled but neither score_tree_interval or score_each_iteration are defined. Early stopping will not be reproducible!\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "███████████████████████████████████████████████ (cancelled)  94%\n"
     ]
    },
    {
     "ename": "H2OJobCancelled",
     "evalue": "Job<$03017f00000146d4ffffffff$_b2cf6635c85c5db671051abc015d4734> was cancelled by the user.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OJobCancelled\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgbm_lat_sin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_lat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_lat_sin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data_shifted_without_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/estimators/estimator_base.py:107\u001b[0m, in \u001b[0;36mH2OEstimator.train\u001b[0;34m(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03mTrain the H2O model.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m:param bool verbose: Print scoring history to stdout. Defaults to False.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m parms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_parms(x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, training_frame\u001b[38;5;241m=\u001b[39mtraining_frame, offset_column\u001b[38;5;241m=\u001b[39moffset_column, \n\u001b[1;32m    104\u001b[0m                          fold_column\u001b[38;5;241m=\u001b[39mfold_column, weights_column\u001b[38;5;241m=\u001b[39mweights_column, \n\u001b[1;32m    105\u001b[0m                          validation_frame\u001b[38;5;241m=\u001b[39mvalidation_frame, max_runtime_secs\u001b[38;5;241m=\u001b[39mmax_runtime_secs, \n\u001b[1;32m    106\u001b[0m                          ignored_columns\u001b[38;5;241m=\u001b[39mignored_columns, model_id\u001b[38;5;241m=\u001b[39mmodel_id, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/estimators/estimator_base.py:199\u001b[0m, in \u001b[0;36mH2OEstimator._train\u001b[0;34m(self, parms, verbose)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest_version \u001b[38;5;241m=\u001b[39m rest_ver\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_model_scoring_history\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m model_json \u001b[38;5;241m=\u001b[39m h2o\u001b[38;5;241m.\u001b[39mapi(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET /\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/Models/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (rest_ver, job\u001b[38;5;241m.\u001b[39mdest_key))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_model(job\u001b[38;5;241m.\u001b[39mdest_key, model_json)\n",
      "File \u001b[0;32m~/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/job.py:85\u001b[0m, in \u001b[0;36mH2OJob.poll\u001b[0;34m(self, poll_updates)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# check if failed... and politely print relevant message\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANCELLED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m H2OJobCancelled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob<\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m> was cancelled by the user.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_key)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob, \u001b[38;5;28mdict\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob)):\n",
      "\u001b[0;31mH2OJobCancelled\u001b[0m: Job<$03017f00000146d4ffffffff$_b2cf6635c85c5db671051abc015d4734> was cancelled by the user."
     ]
    }
   ],
   "source": [
    "gbm_lat_sin.train(\n",
    "    x=features_lat,\n",
    "    y=target_lat_sin,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_lat_cos.train(\n",
    "    x=features_lat,\n",
    "    y=target_lat_cos,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_lat_sin = gbm_lat_sin.model_performance(test_data=validation_data_shifted)\n",
    "performance_lat_cos = gbm_lat_cos.model_performance(test_data=validation_data_shifted)\n",
    "\n",
    "\n",
    "# Print the performance metrics\n",
    "print(performance_lat_sin)\n",
    "print(performance_lat_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_long_sin.train(\n",
    "    x=features_long,\n",
    "    y=target_long_sin,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_long_cos.train(\n",
    "    x=features_long,\n",
    "    y=target_long_cos,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_long_sin = gbm_long_sin.model_performance(test_data=validation_data_shifted)\n",
    "performance_long_cos = gbm_long_cos.model_performance(test_data=validation_data_shifted)\n",
    "\n",
    "print(performance_long_sin)\n",
    "print(performance_long_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_predictions_sin = gbm_lat_sin.predict(test_data_with_last_known)\n",
    "lat_predictions_cos = gbm_lat_cos.predict(test_data_with_last_known)\n",
    "\n",
    "test_data_with_predicted_lat = test_data_with_last_known\n",
    "test_data_with_predicted_lat[\"latitude_sin\"] = lat_predictions_sin\n",
    "test_data_with_predicted_lat[\"latitude_cos\"] = lat_predictions_cos\n",
    "\n",
    "long_predictions_sin = gbm_long_sin.predict(test_data_with_predicted_lat)\n",
    "long_predictions_cos = gbm_long_cos.predict(test_data_with_predicted_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sine and cosine values back to radians\n",
    "lat_predictions_sin = lat_predictions_sin.as_data_frame()\n",
    "lat_predictions_cos = lat_predictions_cos.as_data_frame()\n",
    "long_predictions_sin = long_predictions_sin.as_data_frame()\n",
    "long_predictions_cos = long_predictions_cos.as_data_frame()\n",
    "\n",
    "\n",
    "lat_predictions_radians = np.arctan2(lat_predictions_sin, lat_predictions_cos)\n",
    "long_predictions_radians = np.arctan2(long_predictions_sin, long_predictions_cos)\n",
    "\n",
    "# Convert radians to degrees\n",
    "lat_predictions_degrees = np.rad2deg(lat_predictions_radians)\n",
    "long_predictions_degrees = np.rad2deg(long_predictions_radians)\n",
    "\n",
    "# Print the first few rows to verify the conversion\n",
    "print(lat_predictions_degrees.head())\n",
    "print(long_predictions_degrees.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_visualization_data(validation_data):\n",
    "    lat_val_sin = gbm_lat_sin.predict(validation_data)\n",
    "    lat_val_cos = gbm_lat_cos.predict(validation_data)\n",
    "    long_val_sin = gbm_long_sin.predict(validation_data)\n",
    "    long_val_cos = gbm_long_cos.predict(validation_data)\n",
    "\n",
    "    lat_val_sin = lat_val_sin.as_data_frame()\n",
    "    lat_val_cos = lat_val_cos.as_data_frame()\n",
    "    long_val_sin = long_val_sin.as_data_frame()\n",
    "    long_val_cos = long_val_cos.as_data_frame()\n",
    "\n",
    "    validation_data = validation_data.as_data_frame()\n",
    "\n",
    "    lat_val_radians = np.arctan2(lat_val_sin, lat_val_cos)\n",
    "    long_val_radians = np.arctan2(long_val_sin, long_val_cos)\n",
    "\n",
    "    evaluation_lat_radians = np.arctan2(\n",
    "        validation_data[\"latitude_sin\"], validation_data[\"latitude_cos\"]\n",
    "    )\n",
    "    evaluation_long_radians = np.arctan2(\n",
    "        validation_data[\"longitude_sin\"], validation_data[\"longitude_cos\"]\n",
    "    )\n",
    "\n",
    "    # Convert radians to degrees\n",
    "    lat_val_degrees = np.rad2deg(lat_val_radians)\n",
    "    long_val_degrees = np.rad2deg(long_val_radians)\n",
    "\n",
    "    evaluation_lat_degrees = np.rad2deg(evaluation_lat_radians)\n",
    "    evaluation_long_degrees = np.rad2deg(evaluation_long_radians)\n",
    "\n",
    "    eval_predictions = pd.concat([lat_val_degrees, long_val_degrees], axis=1)\n",
    "\n",
    "    eval_actual = pd.concat([evaluation_lat_degrees, evaluation_long_degrees], axis=1)\n",
    "\n",
    "    eval_predictions.columns = [\"latitude_predicted\", \"longitude_predicted\"]\n",
    "    eval_actual.columns = [\"latitude\", \"longitude\"]\n",
    "\n",
    "    eval = pd.DataFrame()\n",
    "    eval[[\"latitude_predicted\", \"longitude_predicted\"]] = eval_predictions\n",
    "    eval[[\"latitude\", \"longitude\"]] = eval_actual\n",
    "    eval[[\"vesselId\", \"time\"]] = validation_data[[\"vesselId\", \"time\"]]\n",
    "    eval.to_csv(\"eval_predictions.csv\")\n",
    "\n",
    "\n",
    "create_prediction_visualization_data(validation_data_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat([lat_predictions_degrees, long_predictions_degrees], axis=1)\n",
    "predictions.columns = [\"latitude_predicted\", \"longitude_predicted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"ID\"] = test_data[\"ID\"]\n",
    "predictions = predictions[[\"ID\", \"longitude_predicted\", \"latitude_predicted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
