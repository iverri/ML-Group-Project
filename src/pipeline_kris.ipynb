{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import h2o\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# from .feature_engineering_filter import Find_correct_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"17.0.1\" 2021-10-19; OpenJDK Runtime Environment Temurin-17.0.1+12 (build 17.0.1+12); OpenJDK 64-Bit Server VM Temurin-17.0.1+12 (build 17.0.1+12, mixed mode, sharing)\n",
      "  Starting server from /Users/kristofferseyffarth/Downloads/lokalFiles/emner/Maskinlering/Gruppe/.venv/lib/python3.12/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/3t/hy3nmqqx6f70nkbvw0n8lbh80000gn/T/tmpz59om57l\n",
      "  JVM stdout: /var/folders/3t/hy3nmqqx6f70nkbvw0n8lbh80000gn/T/tmpz59om57l/h2o_kristofferseyffarth_started_from_python.out\n",
      "  JVM stderr: /var/folders/3t/hy3nmqqx6f70nkbvw0n8lbh80000gn/T/tmpz59om57l/h2o_kristofferseyffarth_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54339\n",
      "Connecting to H2O server at http://127.0.0.1:54339 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Oslo</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.5</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>1 month and 20 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_kristofferseyffarth_t6cgqj</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>4 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54339</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.12.5 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------\n",
       "H2O_cluster_uptime:         03 secs\n",
       "H2O_cluster_timezone:       Europe/Oslo\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.5\n",
       "H2O_cluster_version_age:    1 month and 20 days\n",
       "H2O_cluster_name:           H2O_from_python_kristofferseyffarth_t6cgqj\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    4 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54339\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.12.5 final\n",
       "--------------------------  ------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init(max_mem_size=\"4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../Datasets/ais_train.csv\", delimiter=\"|\")\n",
    "test_data = pd.read_csv(\"../Datasets/ais_test.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"time\"] = pd.to_datetime(train_data[\"time\"])\n",
    "test_data[\"time\"] = pd.to_datetime(test_data[\"time\"])\n",
    "train_data[\"navstat\"] = train_data[\"navstat\"].astype(\"category\")\n",
    "train_data_preprocessed = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time   sog  rot navstat       etaRaw  \\\n",
      "0 2024-01-01 00:00:25   0.7    0       0  01-09 23:00   \n",
      "1 2024-01-01 00:00:36   0.0   -6       1  12-29 20:00   \n",
      "2 2024-01-01 00:01:45  11.0    0       0  01-02 09:00   \n",
      "3 2024-01-01 00:03:11   0.0    0       1  12-31 20:00   \n",
      "4 2024-01-01 00:03:51  19.7    0       0  01-25 12:00   \n",
      "\n",
      "                   vesselId                    portId  latitude_sin  \\\n",
      "0  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f     -0.569906   \n",
      "1  61e9f3d4b937134a3c4bff1f  634c4de270937fc01c3a7689      0.154614   \n",
      "2  61e9f436b937134a3c4c0131  61d3847bb7b7526e1adf3d19      0.631903   \n",
      "3  61e9f3b4b937134a3c4bfe77  61d36f770a1807568ff9a126     -0.565138   \n",
      "4  61e9f41bb937134a3c4c0087  634c4de270937fc01c3a74f3      0.586143   \n",
      "\n",
      "   latitude_cos  longitude_sin  longitude_cos   cog_sin   cog_cos  \\\n",
      "0      0.821710      -0.846670       0.532118 -0.846670  0.532118   \n",
      "1      0.987975      -0.983189       0.182589 -0.983189  0.182589   \n",
      "2      0.775048      -0.972271       0.233858 -0.972271  0.233858   \n",
      "3      0.824996       0.484494      -0.874795  0.484494 -0.874795   \n",
      "4      0.810208      -0.103077       0.994673 -0.103077  0.994673   \n",
      "\n",
      "   heading_sin  heading_cos  \n",
      "0    -0.846670     0.532118  \n",
      "1    -0.983189     0.182589  \n",
      "2    -0.972271     0.233858  \n",
      "3     0.484494    -0.874795  \n",
      "4    -0.103077     0.994673  \n"
     ]
    }
   ],
   "source": [
    "train_latitude_radians = np.deg2rad(train_data[\"latitude\"])\n",
    "train_longitude_radians = np.deg2rad(train_data[\"longitude\"])\n",
    "train_cog_radians = np.deg2rad(train_data[\"longitude\"])\n",
    "train_heading_radians = np.deg2rad(train_data[\"longitude\"])\n",
    "\n",
    "\n",
    "train_latitude_sin = np.sin(train_latitude_radians)\n",
    "train_latitude_cos = np.cos(train_latitude_radians)\n",
    "\n",
    "train_longitude_sin = np.sin(train_longitude_radians)\n",
    "train_longitude_cos = np.cos(train_longitude_radians)\n",
    "\n",
    "train_cog_sin = np.sin(train_cog_radians)\n",
    "train_cog_cos = np.cos(train_cog_radians)\n",
    "\n",
    "train_heading_sin = np.sin(train_heading_radians)\n",
    "train_heading_cos = np.cos(train_heading_radians)\n",
    "\n",
    "\n",
    "train_data_preprocessed[\"latitude_sin\"] = train_latitude_sin\n",
    "train_data_preprocessed[\"latitude_cos\"] = train_latitude_cos\n",
    "train_data_preprocessed[\"longitude_sin\"] = train_longitude_sin\n",
    "train_data_preprocessed[\"longitude_cos\"] = train_longitude_cos\n",
    "train_data_preprocessed[\"cog_sin\"] = train_cog_sin\n",
    "train_data_preprocessed[\"cog_cos\"] = train_cog_cos\n",
    "train_data_preprocessed[\"heading_sin\"] = train_heading_sin\n",
    "train_data_preprocessed[\"heading_cos\"] = train_heading_cos\n",
    "\n",
    "train_data_preprocessed = train_data_preprocessed.drop(\n",
    "    columns=[\"latitude\", \"longitude\", \"cog\", \"heading\"], axis=1\n",
    ")\n",
    "print(train_data_preprocessed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Last_known_location_training_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"_summary_  Groups training data by vesselId, and propogates all data from last known location\n",
    "\n",
    "    Args:\n",
    "    data (_type_): _description_ the data to be altered\n",
    "\n",
    "    Returns:\n",
    "        _type_:? _description_ the altered data\n",
    "    \"\"\"\n",
    "\n",
    "    grouped_data = data.groupby(\"vesselId\").apply(lambda x: x.sort_values(\"time\"))\n",
    "\n",
    "    print(grouped_data.index)\n",
    "\n",
    "    grouped_data[\"time_diff\"] = (\n",
    "        grouped_data[\"time\"].diff(-1).dt.total_seconds().abs().fillna(0)\n",
    "    )\n",
    "\n",
    "    original_time_and_id = grouped_data[\n",
    "        [\n",
    "            \"time\",\n",
    "            \"vesselId\",\n",
    "            \"latitude_sin\",\n",
    "            \"latitude_cos\",\n",
    "            \"longitude_sin\",\n",
    "            \"longitude_cos\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    shifted_data = grouped_data.shift(1)\n",
    "    shifted_data[\n",
    "        [\n",
    "            \"last_latitude_sin\",\n",
    "            \"last_latitude_cos\",\n",
    "            \"last_longitude_sin\",\n",
    "            \"last_longitude_cos\",\n",
    "        ]\n",
    "    ] = shifted_data[[\"latitude_sin\", \"latitude_cos\", \"longitude_sin\", \"longitude_cos\"]]\n",
    "\n",
    "    shifted_data[\n",
    "        [\n",
    "            \"time\",\n",
    "            \"vesselId\",\n",
    "            \"latitude_sin\",\n",
    "            \"latitude_cos\",\n",
    "            \"longitude_sin\",\n",
    "            \"longitude_cos\",\n",
    "        ]\n",
    "    ] = original_time_and_id[\n",
    "        [\n",
    "            \"time\",\n",
    "            \"vesselId\",\n",
    "            \"latitude_sin\",\n",
    "            \"latitude_cos\",\n",
    "            \"longitude_sin\",\n",
    "            \"longitude_cos\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Drops all values with nan values\n",
    "    result = shifted_data.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Uncomment the line below if you want to remove the \"time\" column after processing\n",
    "    # data = data.drop(\"time\", axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hy3nmqqx6f70nkbvw0n8lbh80000gn/T/ipykernel_87351/1202695444.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_data = data.groupby(\"vesselId\").apply(lambda x: x.sort_values(\"time\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([( '61e9f38eb937134a3c4bfd8b',  131115),\n",
      "            ( '61e9f38eb937134a3c4bfd8b',  131279),\n",
      "            ( '61e9f38eb937134a3c4bfd8b',  131514),\n",
      "            ( '61e9f38eb937134a3c4bfd8b',  131696),\n",
      "            ( '61e9f38eb937134a3c4bfd8b',  131885),\n",
      "            ( '61e9f38eb937134a3c4bfd8b',  132038),\n",
      "            ( '61e9f38eb937134a3c4bfd8b',  132237),\n",
      "            ( '61e9f38eb937134a3c4bfd8b',  132394),\n",
      "            ( '61e9f38eb937134a3c4bfd8b',  132538),\n",
      "            ( '61e9f38eb937134a3c4bfd8b',  132673),\n",
      "            ...\n",
      "            ('clh6aqawa0007gh0z9h6zi9bo', 1520243),\n",
      "            ('clh6aqawa0007gh0z9h6zi9bo', 1520424),\n",
      "            ('clh6aqawa0007gh0z9h6zi9bo', 1520635),\n",
      "            ('clh6aqawa0007gh0z9h6zi9bo', 1520806),\n",
      "            ('clh6aqawa0007gh0z9h6zi9bo', 1521048),\n",
      "            ('clh6aqawa0007gh0z9h6zi9bo', 1521244),\n",
      "            ('clh6aqawa0007gh0z9h6zi9bo', 1521409),\n",
      "            ('clh6aqawa0007gh0z9h6zi9bo', 1521625),\n",
      "            ('clh6aqawa0007gh0z9h6zi9bo', 1521821),\n",
      "            ('clh6aqawa0007gh0z9h6zi9bo', 1522014)],\n",
      "           names=['vesselId', None], length=1522065)\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "train_data_shifted_df = Last_known_location_training_data(train_data_preprocessed)\n",
    "\n",
    "train_data_shifted_df = train_data_shifted_df.drop(columns=[\"time\"], axis=1)\n",
    "\n",
    "train_data_shifted = h2o.H2OFrame(train_data_shifted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_shifted_without_validation, validation_data_shifted = (\n",
    "    train_data_shifted.split_frame(ratios=[0.8], seed=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_last_known_data_test(\n",
    "    test_data: pd.DataFrame, known_data: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"_summary_  Groups training data by vesselId, and propogates all data from last known location\n",
    "\n",
    "    Args:\n",
    "    data (_type_): _description_ the data to be altered\n",
    "\n",
    "    Returns:\n",
    "        _type_:? _description_ the altered data\n",
    "    \"\"\"\n",
    "\n",
    "    if not test_data[\"vesselId\"].isin(known_data[\"vesselId\"]).all():\n",
    "        missing_vessels = test_data[\n",
    "            ~test_data[\"vesselId\"].isin(known_data[\"vesselId\"])\n",
    "        ][\"vesselId\"].unique()\n",
    "        raise ValueError(\n",
    "            f\"The following vesselIds are missing in known_data: {missing_vessels}\"\n",
    "        )\n",
    "    print(\n",
    "        test_data[~test_data[\"vesselId\"].isin(known_data[\"vesselId\"])][\n",
    "            \"vesselId\"\n",
    "        ].unique()\n",
    "    )\n",
    "\n",
    "    grouped_data = (\n",
    "        known_data.sort_values(\"time\")\n",
    "        .groupby(\"vesselId\")\n",
    "        .tail(1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    original_time = test_data[[\"time\"]]\n",
    "    test_data = test_data.drop(\"time\", axis=1)\n",
    "\n",
    "    result = pd.merge(test_data, grouped_data, how=\"left\", on=\"vesselId\")\n",
    "\n",
    "    result[\"time_diff\"] = (original_time[\"time\"] - result[\"time\"]).dt.total_seconds()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "   ID                  vesselId  scaling_factor                time   sog  \\\n",
      "0   0  61e9f3aeb937134a3c4bfe3d             0.3 2024-05-07 23:48:16   0.0   \n",
      "1   1  61e9f473b937134a3c4c02df             0.3 2024-05-07 23:57:16   0.0   \n",
      "2   2  61e9f469b937134a3c4c029b             0.3 2024-05-07 23:59:08  18.7   \n",
      "3   3  61e9f45bb937134a3c4c0221             0.3 2024-05-07 23:52:34   0.1   \n",
      "4   4  61e9f38eb937134a3c4bfd8d             0.3 2024-05-07 23:51:29   0.3   \n",
      "\n",
      "   rot navstat       etaRaw                    portId   cog_sin   cog_cos  \\\n",
      "0    0       5  05-06 10:45  61d38499b7b7526e1adf3d54 -0.989010  0.147846   \n",
      "1    0       5  05-01 23:00  61d37d5799db2ccf7339ef3b  0.863429 -0.504471   \n",
      "2    0       0  05-08 12:45  61d3781293c6feb83e5eb73b  0.187086  0.982343   \n",
      "3    0       1  05-07 01:15  61d37bfe99db2ccf7339ece3  0.124723 -0.992192   \n",
      "4    0       2  05-09 04:00  61d3743d3aeaecc07011a6fa -0.106612  0.994301   \n",
      "\n",
      "   heading_sin  heading_cos  time_diff  last_latitude_sin  last_latitude_cos  \\\n",
      "0    -0.989010     0.147846      900.0           0.517228           0.855848   \n",
      "1     0.863429    -0.504471      541.0           0.255732           0.966748   \n",
      "2     0.187086     0.982343      654.0           0.619491           0.785004   \n",
      "3     0.124723    -0.992192     1080.0          -0.688834           0.724919   \n",
      "4    -0.106612     0.994301     1258.0           0.749340           0.662186   \n",
      "\n",
      "   last_longitude_sin  last_longitude_cos  \n",
      "0           -0.989010            0.147846  \n",
      "1            0.863429           -0.504471  \n",
      "2            0.187086            0.982343  \n",
      "3            0.124723           -0.992192  \n",
      "4           -0.106612            0.994301  \n"
     ]
    }
   ],
   "source": [
    "test_data_with_last_known_df = append_last_known_data_test(\n",
    "    test_data, train_data_preprocessed\n",
    ")\n",
    "test_data_with_last_known_df[\n",
    "    [\n",
    "        \"last_latitude_sin\",\n",
    "        \"last_latitude_cos\",\n",
    "        \"last_longitude_sin\",\n",
    "        \"last_longitude_cos\",\n",
    "    ]\n",
    "] = test_data_with_last_known_df[\n",
    "    [\n",
    "        \"latitude_sin\",\n",
    "        \"latitude_cos\",\n",
    "        \"longitude_sin\",\n",
    "        \"longitude_cos\",\n",
    "    ]\n",
    "]\n",
    "test_data_with_last_known_df = test_data_with_last_known_df.drop(\n",
    "    columns=[\n",
    "        \"latitude_sin\",\n",
    "        \"latitude_cos\",\n",
    "        \"longitude_sin\",\n",
    "        \"longitude_cos\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "test_data_with_last_known = h2o.H2OFrame(test_data_with_last_known_df)\n",
    "print(test_data_with_last_known_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_last_known_df.to_csv(\"../Datasets/test_data_with_last_known.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lat = [\n",
    "    \"vesselId\",\n",
    "    \"cog_sin\",\n",
    "    \"cog_cos\",\n",
    "    \"sog\",\n",
    "    \"rot\",\n",
    "    \"heading_sin\",\n",
    "    \"heading_cos\",\n",
    "    \"navstat\",\n",
    "    \"time_diff\",\n",
    "    \"last_latitude_sin\",\n",
    "    \"last_latitude_cos\",\n",
    "    \"last_longitude_sin\",\n",
    "    \"last_longitude_cos\",\n",
    "]\n",
    "features_long = [\n",
    "    \"vesselId\",\n",
    "    \"cog_sin\",\n",
    "    \"cog_cos\",\n",
    "    \"sog\",\n",
    "    \"rot\",\n",
    "    \"heading_sin\",\n",
    "    \"heading_cos\",\n",
    "    \"navstat\",\n",
    "    \"time_diff\",\n",
    "    \"last_latitude_sin\",\n",
    "    \"last_latitude_cos\",\n",
    "    \"last_longitude_sin\",\n",
    "    \"last_longitude_cos\",\n",
    "    \"predicted_latitude_sin\",\n",
    "    \"predicted_latitude_cos\",\n",
    "]\n",
    "target_long_sin = \"longitude_sin\"\n",
    "target_long_cos = \"longitude_cos\"\n",
    "target_lat_sin = \"latitude_sin\"\n",
    "target_lat_cos = \"latitude_cos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"ntrees\": 300,  # Maximum number of trees\n",
    "    \"max_depth\": 10,  # Maximum depth of each tree\n",
    "    \"min_rows\": 15,  # Minimum number of rows per leaf\n",
    "    \"learn_rate\": 0.05,  # Learning rate\n",
    "    \"sample_rate\": 0.9,  # Row sample rate per tree\n",
    "    \"col_sample_rate\": 0.9,  # Column sample rate per tree\n",
    "    \"reg_lambda\": 1.0,  # L2 regularization term\n",
    "    \"reg_alpha\": 0.1,  # L1 regularization term\n",
    "    \"seed\": 42,  # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "gbm_lat_sin = h2o.estimators.H2OXGBoostEstimator(\n",
    "    ntrees=300,  # Maximum number of trees\n",
    "    max_depth=10,  # Maximum depth of each tree\n",
    "    min_rows=15,  # Minimum number of rows per leaf\n",
    "    learn_rate=0.05,  # Learning rate\n",
    "    sample_rate=0.9,  # Row sample rate per tree\n",
    "    col_sample_rate=0.9,  # Column sample rate per tree\n",
    "    reg_lambda=1.0,  # L2 regularization term\n",
    "    reg_alpha=0.1,  # L1 regularization term\n",
    "    seed=42,  # Random seed for reproducibility\n",
    ")\n",
    "gbm_lat_cos = h2o.estimators.H2OXGBoostEstimator(\n",
    "    ntrees=300,  # Maximum number of trees\n",
    "    max_depth=10,  # Maximum depth of each tree\n",
    "    min_rows=15,  # Minimum number of rows per leaf\n",
    "    learn_rate=0.05,  # Learning rate\n",
    "    sample_rate=0.9,  # Row sample rate per tree\n",
    "    col_sample_rate=0.9,  # Column sample rate per tree\n",
    "    reg_lambda=1.0,  # L2 regularization term\n",
    "    reg_alpha=0.1,  # L1 regularization term\n",
    "    seed=42,  # Random seed for reproducibility\n",
    ")\n",
    "gbm_long_sin = h2o.estimators.H2OXGBoostEstimator(\n",
    "    ntrees=300,  # Maximum number of trees\n",
    "    max_depth=15,  # Maximum depth of each tree\n",
    "    min_rows=15,  # Minimum number of rows per leaf\n",
    "    learn_rate=0.05,  # Learning rate\n",
    "    sample_rate=0.9,  # Row sample rate per tree\n",
    "    col_sample_rate=0.9,  # Column sample rate per tree\n",
    "    reg_lambda=1.0,  # L2 regularization term\n",
    "    reg_alpha=0.1,  # L1 regularization term\n",
    "    seed=42,  # Random seed for reproducibility\n",
    ")\n",
    "gbm_long_cos = h2o.estimators.H2OXGBoostEstimator(\n",
    "    ntrees=300,  # Maximum number of trees\n",
    "    max_depth=15,  # Maximum depth of each tree\n",
    "    min_rows=15,  # Minimum number of rows per leaf\n",
    "    learn_rate=0.05,  # Learning rate\n",
    "    sample_rate=0.9,  # Row sample rate per tree\n",
    "    col_sample_rate=0.9,  # Column sample rate per tree\n",
    "    reg_lambda=1.0,  # L2 regularization term\n",
    "    reg_alpha=0.1,  # L1 regularization term\n",
    "    seed=42,  # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# gbm_cog = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_sog = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_rot = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_heading = h2o.estimators.H2OXGBoostEstimator()\n",
    "# gbm_navstat = h2o.estimators.H2OXGBoostEstimator()\n",
    "# # gbm_etaRaw = h2o.esti#mators.H2OXGBoostEstimator() #Remove etaRaw because it requires preprocessing\n",
    "# # gbm_portId = h2o.estimators.H2OXGBoostEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Model Build progress: |██████████████"
     ]
    }
   ],
   "source": [
    "gbm_lat_sin.train(\n",
    "    x=features_lat,\n",
    "    y=target_lat_sin,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_lat_cos.train(\n",
    "    x=features_lat,\n",
    "    y=target_lat_cos,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_lat_sin = gbm_lat_sin.model_performance(test_data=validation_data_shifted)\n",
    "performance_lat_cos = gbm_lat_cos.model_performance(test_data=validation_data_shifted)\n",
    "\n",
    "\n",
    "# Print the performance metrics\n",
    "print(performance_lat_sin)\n",
    "print(performance_lat_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_long_sin.train(\n",
    "    x=features_long,\n",
    "    y=target_long_sin,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_long_cos.train(\n",
    "    x=features_long,\n",
    "    y=target_long_cos,\n",
    "    training_frame=train_data_shifted_without_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_long_sin = gbm_long_sin.model_performance(test_data=validation_data_shifted)\n",
    "performance_long_cos = gbm_long_cos.model_performance(test_data=validation_data_shifted)\n",
    "\n",
    "print(performance_long_sin)\n",
    "print(performance_long_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_predictions_sin = gbm_lat_sin.predict(test_data_with_last_known)\n",
    "lat_predictions_cos = gbm_lat_cos.predict(test_data_with_last_known)\n",
    "\n",
    "test_data_with_predicted_lat = test_data_with_last_known\n",
    "test_data_with_predicted_lat[\"predicted_latitude_sin\"] = lat_predictions_sin\n",
    "test_data_with_predicted_lat[\"predicted_latitude_cos\"] = lat_predictions_cos\n",
    "\n",
    "long_predictions_sin = gbm_long_sin.predict(test_data_with_predicted_lat)\n",
    "long_predictions_cos = gbm_long_cos.predict(test_data_with_predicted_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sine and cosine values back to radians\n",
    "lat_predictions_sin = lat_predictions_sin.as_data_frame()\n",
    "lat_predictions_cos = lat_predictions_cos.as_data_frame()\n",
    "long_predictions_sin = long_predictions_sin.as_data_frame()\n",
    "long_predictions_cos = long_predictions_cos.as_data_frame()\n",
    "\n",
    "\n",
    "lat_predictions_radians = np.arctan2(lat_predictions_sin, lat_predictions_cos)\n",
    "long_predictions_radians = np.arctan2(long_predictions_sin, long_predictions_cos)\n",
    "\n",
    "# Convert radians to degrees\n",
    "lat_predictions_degrees = np.rad2deg(lat_predictions_radians)\n",
    "long_predictions_degrees = np.rad2deg(long_predictions_radians)\n",
    "\n",
    "# Print the first few rows to verify the conversion\n",
    "print(lat_predictions_degrees.head())\n",
    "print(long_predictions_degrees.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat([lat_predictions_degrees, long_predictions_degrees], axis=1)\n",
    "predictions.columns = [\"latitude_predicted\", \"longitude_predicted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"ID\"] = test_data[\"ID\"]\n",
    "predictions = predictions[[\"ID\", \"longitude_predicted\", \"latitude_predicted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
